{
 "metadata": {
  "name": "regular"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "source": [
      "Regresyon, Ridge, Lasso, Capraz Saglama, Regularize Etmek\n",
      "\n",
      "Konumuz regresyon cesitleri, ve ornek veri olarak diyabet hastaligi\n",
      "olan kisilerden alinmis bazi temel verilerle hastaligin bir sene\n",
      "sonraki ilerleme miktari kullanilacak. Regresyon sayesinde temel\n",
      "veriler ile hastaligin ilerlemesi arasinda bir baglanti bulunabilir,\n",
      "bu sayede hem veri aciklanir / daha iyi anlasilir (hangi degisken\n",
      "onemlidir, hangisi degildir), hem de baska bir hastanin temel\n",
      "verilerini kullanarak o hastanin diyabetinin bir sene sonra ne\n",
      "olacagini tahmin etmek mumkun olur. Kullanilan temel veriler kisinin\n",
      "yasi, cinsiyeti, vucut kutle endeksi (body mass index) ortalama\n",
      "tansiyonu ve alti kere alinmis kan serum olcumleridir.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model, cross_validation\n",
      "from pandas import *\n",
      "diabetes = read_csv(\"diabetes.csv\",sep=';')\n",
      "diabetes_y = diabetes['response']\n",
      "diabetes_x = diabetes.drop(\"response\",axis=1)\n",
      "diabetes_x_train = diabetes_x[:-20]\n",
      "diabetes_x_test  = diabetes_x[-20:]\n",
      "diabetes_y_train = diabetes_y[:-20]\n",
      "diabetes_y_test  = diabetes_y[-20:]"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "source": [
      "Ilk once basit regresyonu hatirlayalim (ordinary least regression). Bu\n",
      "teknigi daha once pek cok yonden gorduk. *Lineer Cebir*, *Cok\n",
      "Degiskenli Calculus* ders notlarinda, ve bizim *Uygulamali Matematik*\n",
      "yazilarinin hepsinde bu teknigin turetilmesi mevcut. Formul\n",
      "\n",
      "$$ \\hat{w} = (X^TX) ^{-1} X^{T}y $$\n",
      "\n",
      "Numerik olarak hemen bu hesabi yapabiliriz. Bir hatirlatma: veri\n",
      "setine y ekseninin nerede kesildiginin bulunabilmesi icin suni bir\n",
      "ekstra 'intercept' adli kolon ekleyecegiz, bu kolon iki boyutta\n",
      "$y=ax+c$ formulundeki $c$'nin bulunabilmesi icindir. Pandas ile bu ekstra\n",
      "kolonu eklemek cok basit, ismen mevcut olmayan kolon erisildigi anda\n",
      "o kolon hemen yoktan yaratilir."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy.linalg as la\n",
      "x_tmp = diabetes_x_train.copy()\n",
      "x_tmp['intercept'] = 1\n",
      "xTx = np.dot(x_tmp.T,x_tmp )\n",
      "ws = np.dot(la.inv(xTx),np.dot(x_tmp.T,diabetes_y_train))\n",
      "ws"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array([  3.03499452e-01,  -2.37639315e+02,   5.10530605e+02,\n",
        "         3.27736981e+02,  -8.14131711e+02,   4.92814589e+02,\n",
        "         1.02848453e+02,   1.84606489e+02,   7.43519617e+02,\n",
        "         7.60951724e+01,   1.52764307e+02])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "source": [
      "Ayni hesabi bir de <code>scikit-learn</code> paketini kullanarak\n",
      "yapalim. Bu paketin <code>LinearRegression</code> cagrisi intercept\n",
      "isini otomatik olarak hallediyor, eger intercept olmasin isteseydik,\n",
      "<code>fit_intercept=False</code> diyecektik."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lin = linear_model.LinearRegression()\n",
      "lin.fit(diabetes_x_train, diabetes_y_train)\n",
      "print lin.coef_\n",
      "print \"score\", lin.score(diabetes_x_test, diabetes_y_test), "
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  3.03499452e-01  -2.37639315e+02   5.10530605e+02   3.27736981e+02\n",
        "  -8.14131711e+02   4.92814589e+02   1.02848453e+02   1.84606489e+02\n",
        "   7.43519617e+02   7.60951724e+01]\n",
        "score 0.585075302278\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "source": [
      "Sonuclar birbirine oldukca yakin. Simdi diger tekniklere gelelim.\n",
      "\n",
      "S\u0131rt Regresyonu (Ridge Regression)\n",
      "\n",
      "Klasik regresyon ile\n",
      "\n",
      "$$ \\hat{w} = \\arg \\min_w ||y-Xw||^2  $$\n",
      "\n",
      "problemini cozdugumuzu biliyoruz, ki $||\\cdot||^2$ Oklit normunun\n",
      "karesini temsil ediyor. Fakat bazi durumlarda $X^TX$'in tekil\n",
      "(singular) olmasi mumkun ki boyle bir durumda $(X^TX)^{-1}$'in tersini\n",
      "almamiz mumkun olmazdi. Tekillik ne zaman ortaya cikar? Eger elimizde\n",
      "veri noktasindan daha fazla boyut var ise mesela... Diyelim ki veri\n",
      "olarak 10 tane kolon var, ama sadece 9 tane veri satiri. Sirt\n",
      "Regregyonunun cikis noktasi budur.\n",
      "\n",
      "Fakat ek olarak bu teknik kestirme hesaplarimiza (estimation) bir\n",
      "meyil / yanlilik (bias) eklemek icin de kullanilabilir ve bu meyil\n",
      "tahminlerin / kestirme hesaplarin iyilesmesine faydali olabilir.\n",
      "\n",
      "Meyili nasil ekleriz? Diyelim ki bizim tanimlayacagimiz bir $\\lambda$ ile\n",
      "tum $ws$'lerin toplamina bir ust sinir tanimlayabiliriz. Boylelikle\n",
      "regresyonun bulacagi katsayilarin cok fazla buyumesine bir \"ceza\" getirmis\n",
      "olacagiz, ve bu cezayi iceren regresyon hesabi o cezadan kacinmak icin\n",
      "mecburen bulacagi katsayilari ufak tutacak, hatta bazilarini sifira indirebilecek.\n",
      "Bu azaltmaya istatistikte kuculme (shrinkage) ismi veriliyor. \n",
      "\n",
      "Sirt regresyonu icin bu kucultme soyle\n",
      "\n",
      "$$ \\hat{w}_{sirt} = \\arg \\min_w ( ||y-Xw||^2 + \\lambda||w||^2 )  $$\n",
      "\n",
      "Goruldugu uzere $w$'nin buyuklugunu, bir $\\lambda$ katsayisi uzerinden\n",
      "minimizasyon problemine dahil ettik, boylece diger parametreler ile buyukluk te\n",
      "minimize edilecek. Ustteki tanim siniri tanimlanmamis (unconstrained) bir\n",
      "optimizasyon problemidir. Sinirli olarak\n",
      "\n",
      "$$ \\min_w ||y-Xw||^2  $$\n",
      "\n",
      "$$ \\textit{Su kosula gore (subject to) }||w||^2 \\le \\tau $$\n",
      "\n",
      "ki $\\lambda$ Lagrange carpanidir. Aslinda simdiye kadar ustteki\n",
      "cevrimin tersini gorduk cogunlukla (yani sinirli problemden sinirsiza\n",
      "gitmeyi), bu gidis tarzini gormek te iyi oldu.\n",
      "\n",
      "Neyse bastaki sinirsiz problemi cozmek icin ifadenin gradyanini alalim,\n",
      "\n",
      "$$ \\nabla \\big( ||y-Xw||^2 + \\lambda||w||^2 \\big) $$\n",
      "\n",
      "$$ \\nabla \\big( (y-Xw)^T (y-Xw) + \\lambda w^Tw \\big) $$\n",
      "\n",
      "$$ \\nabla \\big(  (y^T-w^TX^T)(y-Xw) + \\lambda w^Tw  \\big) $$\n",
      "\n",
      "$$ \\nabla ( y^Ty - y^TXw - w^TX^Ty + w^TX^TXw + \\lambda w^Tw   )  $$\n",
      "\n",
      "$$  - y^TX - X^Ty + 2X^TXw + 2\\lambda w   $$\n",
      "\n",
      "$$  - 2X^Ty + 2X^TXw + 2\\lambda w   $$\n",
      "\n",
      "$$   2X^TXw + 2\\lambda w - 2X^Ty   $$\n",
      "\n",
      "$$   2(X^TX + \\lambda I ) w - 2X^Ty   $$\n",
      "\n",
      "Minimizasyon icin ustteki ifadeyi sifira esitleyebiliriz\n",
      "\n",
      "$$   2(X^TX + \\lambda I ) w - 2X^Ty  = 0 $$\n",
      "\n",
      "O zaman\n",
      "\n",
      "$$   (X^TX + \\lambda I ) w = X^Ty  $$\n",
      "\n",
      "$$   \\hat{w} = (X^TX + \\lambda I)^{-1} X^Ty  $$\n",
      "\n",
      "Bu son ifade en az kareler (least squares) yani normal regresyon cozum\n",
      "formulune cok benziyor, sadece ek olarak bir $\\lambda I$ toplama\n",
      "islemi var.  Demek ki sirt regresyonunu kullanmak icin zaten\n",
      "yaptigimiz hesaba, zaten bizim kendimizin karar verdigi bir $\\lambda$\n",
      "uzerinden $\\lambda I$ eklersek, geri kalan tum islemler ayni olacak. \n",
      "\n",
      "Kontrol edelim"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lam = 0.2\n",
      "wridge = np.dot(la.inv(xTx+lam*np.eye(xTx.shape[0])),\\\n",
      "                np.dot(x_tmp.T,diabetes_y_train))\n",
      "wridge"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "array([  16.70807829, -179.42288145,  447.64999897,  285.41866481,\n",
        "        -51.7991733 ,  -75.09876191, -192.46341288,  123.61066573,\n",
        "        387.91385823,  105.53294479,  152.7637018 ])"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "source": [
      "Simdi <code>scikit-learn</code> ile ayni hesabi yapalim"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ridge = linear_model.Ridge(alpha=0.2)\n",
      "ridge.fit(diabetes_x_train, diabetes_y_train) \n",
      "print ridge.score(diabetes_x_test, diabetes_y_test), ridge.coef_"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.553680030106 [  16.69330211 -179.414259    447.63706059  285.40960442  -51.79094255\n",
        "  -75.08327488 -192.45037659  123.60400024  387.91106403  105.55514774]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "source": [
      "Bir yontem daha var, bu yonteme Lasso ismi veriliyor. Lasso'ya gore cezalandirma\n",
      "\n",
      "$$ \\sum_{k=1}^{n} w_k^2 \\le \\lambda $$\n",
      "\n",
      "uzerinden olur. Bu yontemin tum detaylarina simdilik\n",
      "inmeyecegiz.\n",
      "\n",
      "Ornek olarak bir $\\lambda$ ile onun buldugu katsayilara bakalim."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso = linear_model.Lasso(alpha=0.3)\n",
      "lasso.fit(diabetes_x_train, diabetes_y_train)\n",
      "lasso.coef_"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "array([   0.        ,   -0.        ,  497.3407568 ,  199.17441037,\n",
        "         -0.        ,   -0.        , -118.89291549,    0.        ,\n",
        "        430.93795945,    0.        ])"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "source": [
      "Lasso bazi katsayilari sifira indirdi! Bu katsayilarin agirlik verdigi\n",
      "degiskenleri, eger Lasso'ya inanirsak, modelden tamamen atmak mumkundur.\n",
      "\n",
      "Bu arada Sirt ve Lasso yontemlerinin metotlarina \"regularize etmek\n",
      "(regularization)\" ismi de veriliyor. "
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "k-Katlamali Capraz Saglama (k-fold Cross-Validation)\n",
      "\n",
      "Bir yapay ogrenim algoritmasini kullanmadan once veriyi iki parcaya\n",
      "ayirmak ise yarar; bu parcalar tipik olarak egitim verisi (training\n",
      "set) digeri ise test verisi (validation set) olarak\n",
      "isimlendirilir. Isimlerden belli olacagi uzere, algoritma egitim seti\n",
      "uzerinde egitilir; ve basarisi test verisi uzerinden rapor edilir. Bir\n",
      "bakima modelin olusturulmasi bir set uzerindendir, sonra \"al simdi hic\n",
      "gormedigin bir veri seti, bakalim ne yapacaksin\" sorusunun cevabi,\n",
      "saglamasi bu sekilde yapilir.\n",
      "\n",
      "k-Katlamali Capraz Saglama bu iki parcali egitim / test kavramini bir\n",
      "adim oteye tasir. Ufak bir k seceriz, ki bu genellikle 5 ila 10\n",
      "arasinda bir sayi olur, ve tum verimizi rasgele bir sekilde ama k tane\n",
      "ve esit buyuklukte olacak sekilde parcalara ayiririz. Bu parcalara\n",
      "\"katlar (folds)\" ismi verilir bazen (ki isim buradan geliyor). Sonra\n",
      "teker teker her parcayi test verisi yapariz ve geri kalan tum\n",
      "parcalari egitim verisi olarak kullaniriz. Bu islemi tum parcalar icin\n",
      "tekrarlariz.\n",
      "\n",
      "Bu yaklasim niye faydalidir? Cunku veriyi rasgele sekillerde bolup,\n",
      "pek cok yonden egitim / test icin kullaninca verinin herhangi bir\n",
      "sekilde bizi yonlendirmesi / aldatmasi daha az mumkun hale gelir.\n",
      "\n",
      "Ve iste bu ozelligi, ek olarak, capraz saglamayi \"model secmek\" icin\n",
      "vazgecilmez bir arac haline getirir.\n",
      "\n",
      "Model secmek nedir? Model secimi ustteki baglamda optimal bir\n",
      "$\\lambda$ bulmaktir mesela, yani her modeli temsil eden bir $\\lambda$\n",
      "var ise, en iyi $\\lambda$'yi bulmak, en iyi modeli bulmak anlamina\n",
      "geliyor, capraz saglama bunu sagliyor. Capraz saglama icin\n",
      "<code>scikit-learn</code>'un sagladigi fonksiyonlar vardir, once\n",
      "katlari tanimlariz, sonra bu degistirilmis regresyon fonksiyonlarina\n",
      "katlama usulunu geceriz.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k_fold = cross_validation.KFold(n=420, n_folds=7)"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "source": [
      "Katlari ustteki gibi tanimladik. 420 tane veri noktasini 7 kata bol dedik.\n",
      "Simdi bu katlari kullanalim,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ridge_cv = linear_model.RidgeCV(cv=k_fold)\n",
      "ridge_cv.fit(np.array(diabetes_x), np.array(diabetes_y))\n",
      "ridge_cv.alpha_"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "0.10000000000000001"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "source": [
      "Ustteki sonuc $\\lambda = 0.1$'i gosteriyor. Bu $\\lambda$ daha optimalmis\n",
      "demek ki. Lasso icin benzer sekilde"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso_cv = linear_model.LassoCV(cv=k_fold)\n",
      "lasso_cv.fit(diabetes_x, diabetes_y)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "LassoCV(alphas=None, copy_X=True,\n",
        "    cv=sklearn.cross_validation.KFold(n=420, n_folds=7), eps=0.001,\n",
        "    fit_intercept=True, max_iter=1000, n_alphas=100, normalize=False,\n",
        "    precompute='auto', tol=0.0001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso_cv.alpha_"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.0028395871911771854"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso_cv.score(diabetes_x_test, diabetes_y_test) "
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "0.59709035088351881"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "source": [
      "Simdi veri setinin bir kismi uzerinde teker teker hangi algoritmanin\n",
      "daha basarili oldugunu gorelim. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(row):\n",
      "    j = row; i = row-1\n",
      "    new_data = diabetes_x[i:j]\n",
      "    print diabetes_y[i:j], \"lasso\",lasso_cv.predict(new_data), \\\n",
      "    \t    \"ridge\",ridge_cv.predict(new_data), \\\n",
      "      \t    \"linear\",lin.predict(new_data)\t    \n",
      "\n",
      "predict(-2) # sondan ikinci veri satiri\n",
      "predict(-3)\n",
      "predict(-4)\n",
      "predict(-5)\n",
      "predict(-8)\n"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "439    132\n",
        "Name: response lasso [ 122.23614365] ridge [ 127.1821212] linear [ 123.56604986]\n",
        "438    104\n",
        "Name: response lasso [ 101.85154941] ridge [ 108.89678818] linear [ 102.5713971]\n",
        "437    178\n",
        "Name: response lasso [ 192.95669774] ridge [ 189.58095011] linear [ 194.03798086]\n",
        "436    48\n",
        "Name: response lasso [ 52.89039744] ridge [ 57.66611598] linear [ 52.5445869]\n",
        "433    72\n",
        "Name: response lasso [ 60.42852011] ridge [ 66.3661042] linear [ 61.19831285]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "source": [
      "Ustteki sonuclara gore gercek degeri 132 olan 439. satirda lasso\n",
      "122.2, sirt (ridge) 127.1, basit regresyon ise 123.5 bulmus. O veri\n",
      "noktasi icin sirt yontemi daha basarili cikti.\n",
      "\n",
      "Sonuclara bakinca bazen sirt, bazen normal regresyon basarili cikiyor.\n",
      "Hangi yontem kazanmis o zaman? Bir o, o bir bu ondeyse, hangi yontemi\n",
      "kullanacagimizi nasil bilecegiz?\n",
      "\n",
      "Aslinda her seferinde tek bir metotu kullanmak gerekmiyor. Bu metotlari\n",
      "bir takim (ensemble) halinde isletebiliriz. Her test noktasini, her seferinde\n",
      "tum metotlara sorariz, gelen sonuclarin mesela.. ortalamasini aliriz. Bu\n",
      "sekilde tek basina isleyen tum metotlardan tutarli olarak her seferinde daha\n",
      "iyi sonuca ulasacak bir sonuc elde edebiliriz. Zaten Kaggle gibi yarismalarda\n",
      "cogunlukla birinciligi kazanan metotlar bu turden takim yontemlerini kullanan\n",
      "metotlar, mesela Netflix yarismasini kNN ve SVD metotlarini takim halinde\n",
      "isleten bir grup kazandi. \n",
      "\n",
      "Kaynaklar\n",
      "\n",
      "http://www.lx.it.pt/~mtf/Figueiredo_Linear_Regression.pdf\n",
      "\n",
      "http://www.cs.nyu.edu/~mohri/mls/lecture_8.pdf\n",
      "\n",
      "Harrington, P., *Machine Learning in Action*\n",
      "\n",
      "Shalizi, C., Data Analysis from an Elementary Point of View\n"
     ]
    }
   ]
  }
 ]
}