{
 "metadata": {
  "name": "regular"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "source": [
      "Regresyon, Ridge, Lasso ve Capraz Saglama\n",
      "\n",
      "Konumuz regresyon cesitleri, ve ornek veri olarak diyabet hastaligi\n",
      "olan kisilerden alinmis bazi temel verilerle hastaligin bir sene\n",
      "sonraki ilerleme miktari kullanilacak. Regresyon sayesinde temel\n",
      "veriler ile hastaligin ilerlemesi arasinda bir baglanti bulunabilir,\n",
      "bu sayede hem veriyi aciklanir / anlasilir (hangi degisken onemlidir,\n",
      "hangisi degildir), hem de baska bir hastanin temel verilerini\n",
      "kullanarak o hastanin diyabetinin bir sene sonra ne olacagini tahmin\n",
      "etmek mumkun olur.\n",
      "\n",
      "Kullanilan temel veriler kisinin yasi, cinsiyeti, vucut kutle endeksi\n",
      "(body mass index) ortalama tansiyonu ve alti kere alinmis kan serum\n",
      "olcumleridir.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model, cross_validation\n",
      "from pandas import *\n",
      "diabetes = read_csv(\"diabetes.csv\",sep=';')\n",
      "diabetes_y = diabetes['response']\n",
      "diabetes_x = diabetes.drop(\"response\",axis=1)\n",
      "diabetes_x_train = diabetes_x[:-20]\n",
      "diabetes_x_test  = diabetes_x[-20:]\n",
      "diabetes_y_train = diabetes_y[:-20]\n",
      "diabetes_y_test  = diabetes_y[-20:]"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "source": [
      "Ilk once basit regresyonu hatirlayalim (ordinary least regression). Bu\n",
      "teknigi daha once pek cok yonden gorduk. *Lineer Cebir*, *Cok\n",
      "Degiskenli Calculus* ders notlarinda, ve bizim *Uygulamali Matematik*\n",
      "yazilarinin hepsinde bu teknigin turetilmesi mevcut. Formul\n",
      "\n",
      "$$ \\hat{w} = (X^TX) ^{-1} X^{T}y $$\n",
      "\n",
      "Numerik olarak hemen bu hesabi yapabiliriz. Bir hatirlatma: veri\n",
      "setine y ekseninin nerede kesildiginin bulunabilmesi icin suni bir\n",
      "ekstra 'intercept' adli kolon ekleyecegiz, bu kolon iki boyutta\n",
      "$y=ax+c$ formulundeki $c$'nin bulunabilmesi icindir. Pandas ile bu ekstra\n",
      "kolonu eklemek cok basit, ismen mevcut olmayan kolon erisildigi anda\n",
      "o kolon hemen yoktan yaratilir."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy.linalg as la\n",
      "x_tmp = diabetes_x_train.copy()\n",
      "x_tmp['intercept'] = 1\n",
      "xTx = np.dot(x_tmp.T,x_tmp )\n",
      "ws = np.dot(la.inv(xTx),np.dot(x_tmp.T,diabetes_y_train))\n",
      "ws"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "array([  3.03499452e-01,  -2.37639315e+02,   5.10530605e+02,\n",
        "         3.27736981e+02,  -8.14131711e+02,   4.92814589e+02,\n",
        "         1.02848453e+02,   1.84606489e+02,   7.43519617e+02,\n",
        "         7.60951724e+01,   1.52764307e+02])"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "source": [
      "Ayni hesabi bir de <code>scikit-learn</code> paketini kullanarak\n",
      "yapalim. Bu paketin <code>LinearRegression</code> cagrisi intercept\n",
      "isini otomatik olarak hallediyor, eger intercept olmasin isteseydik,\n",
      "<code>fit_intercept=False</code> diyecektik."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lin = linear_model.LinearRegression()\n",
      "lin.fit(diabetes_x_train, diabetes_y_train)\n",
      "print lin.coef_\n",
      "print \"score\", lin.score(diabetes_x_test, diabetes_y_test), "
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  3.03499452e-01  -2.37639315e+02   5.10530605e+02   3.27736981e+02\n",
        "  -8.14131711e+02   4.92814589e+02   1.02848453e+02   1.84606489e+02\n",
        "   7.43519617e+02   7.60951724e+01]\n",
        "score 0.585075302278\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "source": [
      "Sonuclar birbirine oldukca yakin. Simdi diger tekniklere gelelim.\n",
      "\n",
      "S\u0131rt Regresyonu (Ridge Regression)\n",
      "\n",
      "Klasik regresyon ile\n",
      "\n",
      "$$ \\hat{w} = \\arg \\min_w ||y-Xw||^2  $$\n",
      "\n",
      "problemini cozdugumuzu biliyoruz, ki $||\\cdot||^2$ Oklit normunun\n",
      "karesini temsil ediyor. Fakat bazi durumlarda $X^TX$'in tekil\n",
      "(singular) olmasi mumkun ki boyle bir durumda $(X^TX)^{-1}$'in tersini\n",
      "almamiz mumkun olmazdi. Tekillik ne zaman ortaya cikar? Eger elimizde\n",
      "veri noktasindan daha fazla boyut var ise mesela... Diyelim ki veri\n",
      "olarak 10 tane kolon var, ama sadece 9 tane veri satiri. Sirt\n",
      "Regregyonunun cikis noktasi budur.\n",
      "\n",
      "Fakat ek olarak bu teknik kestirme hesaplarimiza (estimation) bir\n",
      "meyil / yanlilik (bias) eklemek icin de kullanilabilir ve bu meyil\n",
      "tahminlerin / kestirme hesaplarin iyilesmesine faydali olabilir.\n",
      "\n",
      "Meyili nasil ekleriz? Diyelim ki bizim tanimlayacagimiz bir $\\lambda$ ile\n",
      "tum $ws$'lerin toplamina bir ust sinir tanimlayabiliriz. Boylelikle\n",
      "regresyonun bulacagi katsayilarin cok fazla buyumesine bir \"ceza\" getirmis\n",
      "olacagiz, ve bu cezayi iceren regresyon hesabi o cezadan kacinmak icin\n",
      "mecburen bulacagi katsayilari ufak tutacak, hatta bazilarini sifira indirebilecek.\n",
      "Bu azaltmaya istatistikte kuculme (shrinkage) ismi veriliyor. \n",
      "\n",
      "Sirt regresyonu icin bu kucultme soyle\n",
      "\n",
      "$$ \\hat{w}_{sirt} = \\arg \\min_w \\big\\{ ||y-Xw||^2 + \\lambda||w||^2 \\big\\}  $$\n",
      "\n",
      "Goruldugu uzere $w$'nin buyuklugunu, bir $\\lambda$ katsayisi uzerinden\n",
      "minimizasyon problemine dahil ettik, boylece diger parametreler ile buyukluk te\n",
      "minimize edilecek. Ustteki tanim siniri tanimlanmamis (unconstrained) bir\n",
      "optimizasyon problemidir. Sinirli olarak\n",
      "\n",
      "$$ \\min_w ||y-Xw||^2  $$\n",
      "\n",
      "$$ \\textit{Su kosula gore (subject to) }||w||^2 \\le \\tau $$\n",
      "\n",
      "ki $\\lambda$ Lagrange carpanidir. Aslinda simdiye kadar ustteki\n",
      "cevrimin tersini gorduk cogunlukla (yani sinirli problemden sinirsiza\n",
      "gitmeyi), bu gidis tarzini gormek te iyi oldu.\n",
      "\n",
      "Neyse bastaki sinirsiz problemi cozmek icin ifadenin gradyanini alalim,\n",
      "\n",
      "$$ \\nabla \\big( ||y-Xw||^2 + \\lambda||w||^2 \\big) $$\n",
      "\n",
      "$$ \\nabla \\big( (y-Xw)^T (y-Xw) + \\lambda w^Tw \\big) $$\n",
      "\n",
      "$$ \\nabla \\big(  (y^T-w^TX^T)(y-Xw) + \\lambda w^Tw  \\big) $$\n",
      "\n",
      "$$ \\nabla ( y^Ty - y^TXw - w^TX^Ty + w^TX^TXw + \\lambda w^Tw   )  $$\n",
      "\n",
      "$$  - y^TX - X^Ty + 2X^TXw + 2\\lambda w   $$\n",
      "\n",
      "$$  - 2X^Ty + 2X^TXw + 2\\lambda w   $$\n",
      "\n",
      "$$   2X^TXw + 2\\lambda w - 2X^Ty   $$\n",
      "\n",
      "$$   2(X^TX + \\lambda I ) w - 2X^Ty   $$\n",
      "\n",
      "Minimizasyon icin ustteki ifadeyi sifira esitleyebiliriz\n",
      "\n",
      "$$   2(X^TX + \\lambda I ) w - 2X^Ty  = 0 $$\n",
      "\n",
      "O zaman\n",
      "\n",
      "$$   (X^TX + \\lambda I ) w = X^Ty  $$\n",
      "\n",
      "$$   \\hat{w} = (X^TX + \\lambda I)^{-1} X^Ty  $$\n",
      "\n",
      "Bu son ifade en az kareler (least squares) yani normal regresyon cozum\n",
      "formulune cok benziyor, sadece ek olarak bir $\\lambda I$ toplama\n",
      "islemi var.  Demek ki sirt regresyonunu kullanmak icin zaten\n",
      "yaptigimiz hesaba, zaten bizim kendimizin karar verdigi bir $\\lambda$\n",
      "uzerinden $\\lambda I$ eklersek, geri kalan tum islemler ayni olacak. \n",
      "\n",
      "Kontrol edelim"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lam = 0.2\n",
      "wridge = np.dot(la.inv(xTx+lam*np.eye(xTx.shape[0])),\\\n",
      "                np.dot(x_tmp.T,diabetes_y_train))\n",
      "wridge"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "array([  16.70807829, -179.42288145,  447.64999897,  285.41866481,\n",
        "        -51.7991733 ,  -75.09876191, -192.46341288,  123.61066573,\n",
        "        387.91385823,  105.53294479,  152.7637018 ])"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "source": [
      "Simdi <code>scikit-learn</code> ile ayni hesabi yapalim"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ridge = linear_model.Ridge(alpha=0.2)\n",
      "ridge.fit(diabetes_x_train, diabetes_y_train) \n",
      "print ridge.score(diabetes_x_test, diabetes_y_test), ridge.coef_"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.553680030106 [  16.69330211 -179.414259    447.63706059  285.40960442  -51.79094255\n",
        "  -75.08327488 -192.45037659  123.60400024  387.91106403  105.55514774]\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "source": [
      "Bir yontem daha var, bu yonteme Lasso ismi veriliyor. Lasso'ya gore cezalandirma\n",
      "\n",
      "$$ \\sum_{k=1}^{n} w_k^2 \\le \\lambda $$\n",
      "\n",
      "uzerinden olur. Bu yontemin tum detaylarina simdilik\n",
      "inmeyecegiz.\n",
      "\n",
      "Ornek olarak bir $\\lambda$ ile onun buldugu katsayilara bakalim."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso = linear_model.Lasso(alpha=0.3)\n",
      "lasso.fit(diabetes_x_train, diabetes_y_train)\n",
      "lasso.coef_"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "array([   0.        ,   -0.        ,  497.3407568 ,  199.17441037,\n",
        "         -0.        ,   -0.        , -118.89291549,    0.        ,\n",
        "        430.93795945,    0.        ])"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "source": [
      "Lasso bazi katsayilari sifira indirdi! Bu katsayilarin agirlik verdigi\n",
      "degiskenleri, eger Lasso'ya inanirsak, modelden tamamen atmak mumkundur. "
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "k-Katlamali Capraz Saglama (k-fold Cross-Validation)\n",
      "\n",
      "Bir yapay ogrenim algoritmasini kullanmadan once veriyi iki parcaya\n",
      "ayirmak ise yarar; bu parcalar tipik olarak egitim verisi (training\n",
      "set) digeri ise test verisi (validation set) olarak\n",
      "isimlendirilir. Isimlerden belli olacagi uzere, algoritma egitim seti\n",
      "uzerinde egitilir; ve basarisi test verisi uzerinden rapor edilir. Bir\n",
      "bakima modelin olusturulmasi bir set uzerindendir, sonra \"al simdi hic\n",
      "gormedigin bir veri seti, bakalim ne yapacaksin\" sorusunun cevabi,\n",
      "saglamasi bu sekilde yapilir.\n",
      "\n",
      "k-Katlamali Capraz saglama bu iki parcali egitim / test kavramini bir\n",
      "adim oteye tasir. Ufak bir k seceriz, ki bu genellikle 5 ila 10\n",
      "arasinda bir sayi olur, ve tum verimizi rasgele bir sekilde ama k tane\n",
      "ve esit buyuklukte olacak sekilde parcalara ayiririz. Bu parcalara\n",
      "\"katlar (folds)\" ismi verilir bazen (ki isim buradan geliyor). Sonra\n",
      "teker teker her parcayi bir kere test verisi yapariz ve geri kalan tum\n",
      "parcalari egitim verisi olarak kullaniriz.\n",
      "\n",
      "Bu yaklasim niye faydalidir? Cunku veriyi rasgele sekillerde bolup,\n",
      "pek cok yonden egitim / test icin kullaninca verinin herhangi bir\n",
      "yonde bizi aldatmasi daha az mumkun hale gelir. Iste bu ozelligi\n",
      "capraz saglamayi \"model secmek\" icin vazgecilmez bir arac haline\n",
      "getirir.\n",
      "\n",
      "Model secmek nedir? Model secimi ustteki baglamda optimal bir\n",
      "$\\lambda$ bulmaktir mesela. Capraz saglama icin <code>scikit-learn</code>'un\n",
      "sagladigi fonksiyonlar vardir, once katlari tanimlariz, sonra bu degistirilmis\n",
      "regresyon fonksiyonlarina katlama usulunu geceriz. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k_fold = cross_validation.KFold(n=420, n_folds=12)"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ridge_cv = linear_model.RidgeCV(cv=k_fold)\n",
      "ridge_cv.fit(np.array(diabetes_x), np.array(diabetes_y))\n",
      "ridge_cv.alpha_"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "0.10000000000000001"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "source": [
      "Ustteki sonuc $\\lambda = 0.1$'i gosteriyor. Bu $\\lambda$ daha optimalmis\n",
      "demek ki. Lasso icin benzer sekilde"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso_cv = linear_model.LassoCV(cv=k_fold)\n",
      "lasso_cv.fit(diabetes_x, diabetes_y)"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 69,
       "text": [
        "LassoCV(alphas=None, copy_X=True,\n",
        "    cv=sklearn.cross_validation.KFold(n=420, n_folds=12), eps=0.001,\n",
        "    fit_intercept=True, max_iter=1000, n_alphas=100, normalize=False,\n",
        "    precompute='auto', tol=0.0001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso_cv.alpha_"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "0.057053922981741868"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso_cv.score(diabetes_x_test, diabetes_y_test) "
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 71,
       "text": [
        "0.59694484872735032"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "source": [
      "Simdi veri setinin bir kismi uzerinde teker teker hangi algoritmanin\n",
      "daha basarili oldugunu gorelim. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(row):\n",
      "    j = row; i = row-1\n",
      "    new_data = diabetes_x[i:j]\n",
      "    print diabetes_y[i:j], \"lasso\",lasso_cv.predict(new_data), \\\n",
      "    \t    \"ridge\",ridge_cv.predict(new_data), \\\n",
      "      \t    \"linear\",lin.predict(new_data)\t    \n",
      "\n",
      "predict(-2) # sondan ikinci veri satiri\n",
      "predict(-3)\n",
      "predict(-4)\n",
      "predict(-5)\n",
      "predict(-6)\n"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "439    132\n",
        "Name: response lasso [ 125.28611712] ridge [ 127.1821212] linear [ 123.56604986]\n",
        "438    104\n",
        "Name: response lasso [ 109.29237355] ridge [ 108.89678818] linear [ 102.5713971]\n",
        "437    178\n",
        "Name: response lasso [ 193.36448285] ridge [ 189.58095011] linear [ 194.03798086]\n",
        "436    48\n",
        "Name: response lasso [ 55.57075543] ridge [ 57.66611598] linear [ 52.5445869]\n",
        "435    64\n",
        "Name: response lasso [ 120.58731424] ridge [ 121.47800128] linear [ 120.33329252]\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "source": [
      "Ustteki sonuclara gore gercek degeri 132 olan 439. satirda lasso\n",
      "125.2, sirt (ridge) 127.1, basit regresyon ise 123.5 bulmus. O veri\n",
      "noktasis icin sirt yontemi daha basarili cikti."
     ]
    },
    {
     "cell_type": "markdown",
     "source": [
      "Kaynaklar\n",
      "\n",
      "http://www.lx.it.pt/~mtf/Figueiredo_Linear_Regression.pdf\n",
      "\n",
      "http://www.cs.nyu.edu/~mohri/mls/lecture_8.pdf\n",
      "\n",
      "Harrington, P., *Machine Learning in Action*\n",
      "\n",
      "Shalizi, C., Data Analysis from an Elementary Point of View\n"
     ]
    }
   ]
  }
 ]
}