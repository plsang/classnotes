\documentclass[12pt,fleqn]{article}\usepackage{../common}
\begin{document}
Matris Türevleri

Aksi belirtilmedikce altta $a,x$ gibi vektorler kolon vektorleri olacaktir,
yani $m \times 1$, ya da $n \times 1$ gibi boyutlara sahip olacaklardir. 

Skalar Fonksiyon Turevi, Gradyan

$m$ boyutlu vektor $x$'i alan ve geriye tek sayi sonucu donduren bir $f(x)$
fonksiyonunun $x$'e gore turevini nasil aliriz? Yani $x \in \mathbb{R}^m$
ve bir vektor,

$$ x = 
\left[\begin{array}{ccc}
x_1 \\ \vdots \\ x_m
\end{array}\right]
 $$

Bu durumda $x$'in her hucresine / ogesine gore kismi turevler (partial
derivatives) alinir, sonucta tek boyutlu / tekil sayili fonksiyon, turev 
sonrasi $m$ boyutlu bir sonuc vektorunu yaratir, yani

$$
\frac{\partial f}{\partial x}  =
\left[\begin{array}{c}
\frac{\partial f}{\partial x_1} \\
\\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_m} 
\end{array}\right]
$$

Bu sonuc tanidik gelmis olabilir, bu ifade gradyan olarak ta bilinir. 

$$ \frac{\partial f}{\partial x}  = \nabla f = grad \ f(x) $$

Turev bir kolon vektoru olarak cikti cunku $x$ de bir kolon
vektoruydu. Elde edilen vektor surpriz degil cunku tek, skalar bir deger
veren bir fonksiyonun $x$ icindeki {\em her ogensinin} nasil degistigine
gore bunun fonksiyon uzerindeki etkilerini merak ediyorduk, ustteki vektor
oge bazinda bize aynen bunu gosteriyor. Yani tek skalar sonuc $m$ tane
turev sonucuna ayriliyor, cunku tek sonucun $m$ tane secenege gore
degisimini gormek istedik. 

Not olarak belirtelim, gradyan vektoru matematiksel bir kisayoldur, yani
matematikte kuramsal olarak turetilerek ulasilan ana kurallardan biri
denemez. Fakat cok ise yaradigina suphe yok.

Skalar Parametreye Gore Matris Turevi

Eger bir $A$ matrisinin tum ogeleri tek sayi $\theta$ gibi bir degiskene
bagli ise, o matrisin $\theta$'ya gore turevi, tum elemanlarinin teker
teker $\theta$'ya gore turevidir,

$$ 
\frac{\partial A}{\partial \theta} = 
\left[\begin{array}{cccc}
\frac{\partial a_{11}}{\partial \theta} & 
\frac{\partial a_{12}}{\partial \theta} & \dots & 
\frac{\partial a_{1n}}{\partial \theta} \\

\frac{\partial a_{21}}{\partial \theta} & 
\frac{\partial a_{22}}{\partial \theta} &  \dots & 
\frac{\partial a_{2n}}{\partial \theta}  \\

\vdots & \vdots & \ddots & \vdots \\

\frac{\partial a_{m1}}{\partial \theta} & 
\frac{\partial a_{m2}}{\partial \theta} &  \dots & 
\frac{\partial a_{mn}}{\partial \theta}  

\end{array}\right]
$$

Cok Parametreli Matris Turevi

Simdi ilginc bir durum; diyelim ki hem fonksiyon $f(x)$'e verilen $x$
cok boyutlu, hem de fonksiyonun sonucu cok boyutlu! Bu gayet mumkun bir
durum. Bu durumda ne olurdu? 

Eger $f$'in turevinin her turlu degisimi temsil etmesini istiyorsak, o
zaman hem her girdi hucresi, hem de her cikti hucresi kombinasyonu icin bu
degisimi saptamaliyiz. Jacobian matrisleri tam da bunu yapar. Eger $m$
boyutlu girdi ve $n$ boyutlu cikti tanimlayan $f$'in turevini almak
istersek, bu bize $m \times n$ boyutunda bir matris verecektir!
Hatirlarsak daha once gradyan sadece $m$ boyutunda bir {\em vektor}
vermisti. Simdi sonuc bir matris. 

$$ 
J(x) = \frac{\partial f(x)}{\partial x} =
\left[\begin{array}{ccc}
\frac{\partial f_{1}(x)}{\partial \theta} & \dots & 
\frac{\partial f_{1}(x)}{\partial \theta} \\

\vdots & \ddots & \vdots \\

\frac{\partial f_{n}(x)}{\partial \theta} & \dots & 
\frac{\partial f_{n}(x)}{\partial \theta}  
\end{array}\right]
 $$

Vektor Turevleri

$a,x \in \mathbb{R}^n$ ise, $a^Tx$'in $x$'e gore turevi nedir? 

$a^Tx$ bir noktasal carpim olduguna gore sonucu bir tek sayidir
(scalar). Bu noktasal carpimi bir fonksiyon gibi dusunebiliriz, bu durumda
demektir ki tek sayili bir fonksiyonun cok boyutlu $x$'e gore turevini
aliyoruz, yani gradyan durumu tekrar vuku buldu,

$$ 
\frac{\partial (a^Tx)}{\partial x} = 
\left[\begin{array}{c}
\frac{\partial (a^Tx)}{\partial x_1} \\ 
\vdots \\ 
\frac{\partial (a^Tx)}{\partial x_n} 
\end{array}\right] 
 $$

$$  =
\left[\begin{array}{c}
\frac{\partial (a_1x_1 + ... + a_nx_n)}{\partial x_1} \\ 
\vdots \\ 
\frac{\partial (a_1x_1 + ... + a_nx_n)}{\partial x_n} 
\end{array}\right] 
= 
\left[\begin{array}{c}
a_1 \\
\vdots  \\
a_n
\end{array}\right] =
a
 $$


Niye her satirda $a_1,a_2$  gibi degerler elde ettigimizin sebebi bariz
herhalde, cunku mesela ilk satirda $x_1$'e gore turev alindigi durumda
$a_1x_1$ haricindeki tum terimler yokolacaktir, cunku o terimler icinde
$x_1$ yoktur ve Calculus'a gore sabit sayi sayilirlar.

Peki $a^Tx$'in $x^T$'ye gore turevi nedir? 

$x^T$'nin yatay bir vektor olduguna dikkat, yani satir vektoru, o zaman
sonuc yatay bir vektor olacaktir (kiyasla gradyan dikeydi).

$$ 
\frac{\partial (a^Tx)}{\partial x^T} = 
\left[\begin{array}{ccc}
\frac{\partial (a^Tx)}{\partial x_1} &
\dots 
&
\frac{\partial (a^Tx)}{\partial x_n} 
\end{array}\right] 
\mlabel{1}
 $$

$$ =
\left[\begin{array}{ccc}
a_1 & \dots & a_n
\end{array}\right] = 
a^T $$

Matris Turevleri

Eger bir $x \in \mathbb{R}^m$ vektorunden $A$ matrisi $x$ ile carpiliyor
ise, bu carpimin $x$'e gore turevi nedir? 


$$ \frac{\partial}{\partial x^T} [Ax] = A
$$

Ispat: Eger $a_i \in \mathbb{R}^n$ ise (ki devrigi alininca bu vektor yatay hale
gelir, yani altta bu yatay vektorleri ust uste istifledigimizi dusunuyoruz),

$$ A = \left[\begin{array}{c}
a_1^T \\ \vdots \\ a_m^T
\end{array}\right] $$

O zaman $Ax$ ne olur? {\em Matris Carpimi} yazisindaki ``satir bakis acisi''
dusunulurse, $A$'in her satiri, ayri ayri $x$'in tum satirlarini kombine
ederek tekabul eden sonuc satirini olusturur (tabii bu ornekte $x$'in
kendisi bir vektor o yuzden ``satirlari'' tek bir sayidan ibaret), 

$$ Ax = \left[\begin{array}{c}
a_1^Tx \\ \vdots \\ a_m^Tx
\end{array}\right] 
$$

Ustteki bir vektor, her ogesi tek sayi. Turevi alirsak (dikkat yatay
vektore gore turev aliyoruz), ve (1)'i dikkate alirsak, 

$$ 
\frac{\partial Ax}{\partial x^T}  =
\left[\begin{array}{c}
\frac{\partial(a_1^Tx) }{\partial x^T} 
\\ 
\vdots \\ 
\frac{\partial(a_m^Tx) }{\partial }
\end{array}\right] = \left[\begin{array}{c}
a_1^T \\ \vdots \\ a_m^T
\end{array}\right]  = A
 $$


Kaynaklar

Duda, Hart, {\em Pattern Classification}






Kaynaklar 

Economics 627 Econometric Theory II, Vector and Matrix Differentiation,
\url{http://faculty.arts.ubc.ca/vmarmer/econ627/}
        
Duda, Hart, {\em Pattern Classification}

\end{document}
