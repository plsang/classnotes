\documentclass[12pt,fleqn]{article}\usepackage{../common}
\begin{document}
Naive Bayes

Reel sayilar arasinda baglanti kurmak icin istatistikte regresyon
kullanilir. Eger reel degerleri, (mesela) iki kategorik grup arasinda secmek
icin kullanmak istenirse, bunun icin lojistik regresyon gibi teknikler de
vardir.

Fakat kategoriler / gruplar ile baska kategorik gruplar arasinda
baglantilar kurulmak istenirse, standart istatistik yontemleri faydali
olamiyor. Bu gibi ihtiyaclar icin makine ogrenimi (machine learning)
dunyasindan Naive Bayes gibi tekniklere bakmamiz lazim.

Not: Daha ilerlemeden belirtelim, bu teknigin ismi Naive Bayes ama bu tanim
dogru degil, cunku bu teknik Olasilik Teorisi'nden bilinen Bayes Teorisini
kullanmiyor.

Oncelikle kategorik degerler ile ne demek istedigimizi belirtelim. Reel
sayilar $0.3423, 2.4334$ gibi degerlerdir, kategorik degerler ile ise
mesela bir belge icinde 'a','x' gibi harflerin mevcut olmasidir. Ya da, bir
evin 'beyaz', 'gri' renkli olmasi.. Burada oyle kategorilerden bahsediyoruz
ki istesek te onlari sayisal bir degere ceviremiyoruz; kiyasla mesela bir
gunun 'az sicak', 'orta', 'cok sicak' oldugu verisini kategorik bile olsa
regresyon amaciyla sayiya cevirip kullanabilirdik. Az sicak = 0, orta = 1,
cok sicak = 2 degerlerini kullanabilirdik, regresyon hala anlamli olurdu
(cunku arka planda bu kategoriler aslinda sayisal sicaklik degerlerine
tekabul ediyor olurlardi). Fakat 'beyaz', 'gri' degerlere sayi atamanin
regresyon acisindan bir anlami olmazdi, hatta bunu yapmak yanlis
olurdu. Eger elimizde fazla sayida 'gri' ev verisi olsa, bu durum regresyon
sirasinda beyaz evlerin beyazligini mi azaltacaktir?

Iste bu gibi durumlarda kategorileri oldugu gibi isleyebilen bir teknik
gerekiyor. Bu yazida kullanacagimiz ornek, bir belgenin icindeki kelimelere
gore kategorize edilmesi. Elimizde iki turlu dokuman olacak. Bir tanesi
Stephen Hawking adli bilim adaminin bir kitabindan 3 sayfa, digeri baskan
Barack Obama'nin bir kitabindan 3 sayfa. Bu sayfalar ve icindeki kelimeler
NB yontemini ``egitmek'' icin kullanilacak, sonra NB tarafindan hic
gorulmemis yeni sayfalari yontemimize kategorize ettirecegiz.

Cok Boyutlu Bernoulli ve Kelimeler

\includegraphics[height=2cm]{dims.png}

Bir dokuman ile icindeki kelimeler arasinda nasil baglanti kuracagiz?
Burada olasilik teorisinden Cok Boyutlu Bernoulli (Multivariate Bernoulli)
dagilimini kullanacagiz. Ustteki resimde goruldugu gibi her dokuman bir
$x^i$ rasgele degiskeniyle temsil edilecek. Tek boyutlu Bernoulli degiskeni
'1' ya da '0' degerine sahip olabilir, cok boyutlu olani ise bir vektor
icinde '1' ve '0' degerlerini tasiyabilir. Iste bu vektorun her hucresi,
onceden tanimli bir kelimeye tekabul edecek, ve bu kelimeden bir dokuman
icinde en az bir tane var ise, o hucre '1' degerini tasiyacak, yoksa '0'
degerini tasiyacak. Ustteki ornekte 2. kelime ``hello'' ve 4. dokuman
icinde bu kelimeden en az bir tane var, o zaman $x_2^4 = 1$. Tek bir
dokumani temsil eden dagilimi matematiksel olarak soyle yazabiliriz:

\[ p(x_1,...,x_{D}) = \prod_{d=1}^{D} p(x_d)=\prod_{d=1}^{D}
\alpha_d^{x_d}(1-\alpha_d)^{1-x_d} 
 \]

 Bu formulde her $d$ boyutu bir tek boyutlu Bernoulli, ve bir dokuman icin
 tum bu boyutlarin birlesik (joint) dagilimi gerekiyor, carpimin sebebi
 bu. Formuldeki $\alpha_d$ bir dagilimi ``tanimlayan'' deger, $\alpha$ bir
 vektor, ve unutmayalim, her ``sinif'' icin NB ayri ayri egitilecek, ve her
 sinif icin farkli $\alpha$ vektoru olacak. Yani Obama'nin kitaplari icin
 $\alpha_2 = 0.8$ olabilir, Hawking kitabi icin $\alpha_2 = 0.3$
 olabilir. Birinin kitabinda ``hello'' kelimesi olma sansi fazla, digerinde
 pek yok. O zaman NB'yi ``egitmek'' ne demektir? Egitmek her sinif icin
 yukaridaki $\alpha$ degerlerini bulmak demektir.

 Bunun icin istatistikteki ``olurluk (likelihood)'' kavramini kullanmak
 yeterli. Olurluk, bir dagilimdan geldigi farzedilen bir veri setini alir,
 tum veri noktalarini teker teker olasiliga gecerek olasilik degerlerini
 birbirine carpar. Sonuc ne kadar yuksek cikarsa, bu verinin o dagilimdan
 gelme olasiligi o kadar yuksek demektir. Bizim problemimiz icin tek bir
 sinifin olurlugu, o sinif icindeki tum (N tane) belgeyi kapsamalidir, tek
 bir ``veri noktasi'' tek bir belgedir, o zaman:

\[ L(\theta) = \prod_{i=1}^N \prod_{d=1}^{D} p(x_d^i) = 
\prod_{i=1}^N \prod_{d=1}^{D} \alpha_d^{x_d^i}(1-\alpha_d)^{1-x_d^i}
 \]

$\theta$ bir dagilimi tanimlayan her turlu degisken anlaminda kullanildi, bu
ornekte icinde sadece $\alpha$ var.

Devam edelim: Eger $\alpha$'nin ne oldugunu bilmiyorsak (ki bilmiyoruz
-egitmek zaten bu demek-) o zaman maksimum olurluk (maximum likelihood)
kavramini resme dahil etmek gerekli. Bunun icin ustteki olurluk formulunun
$\alpha$'ya gore turevini alip sifira esitlersek, bu formulden bir maksimum
noktasindaki $\alpha$ elimize gececektir. Iste bu $\alpha$ bizim aradigimiz
deger. Veriyi en iyi temsil eden $\alpha$ degeri bu demektir. Onu bulunca
egitim tamamlanir.

Turev almadan once iki tarafin log'unu alalim, boylece carpimlar toplamlara
donusecek ve turevin formulun icine nufuz etmesi daha kolay olacak.

\[ log(L) = \sum_{i=1}^N \sum_{d=1}^{D} {x_d^i}\ log (\alpha_d) + 
(1-x_d^i)\ log (1-\alpha_d) \]

Turevi alalim:

\[ \frac{dlog(L)}{d\alpha_d} = \sum_{i=1}^N \bigg( \frac{x_d^i}{\alpha_d} -
\frac{1-x_d^i}{1-\alpha_d} \bigg) = 0
 \]

1) $\alpha_d$'ye gore turev alirken $x_d^i$'ler sabit sayi gibi muamele
gorurler. 2) log'un turevi alirken log icindeki degerlerin turev alinmis hali
bolumun ustune, kendisini oldugu gibi bolum altina alinir, ornek 
$dlog(-x)/dx = -1/x$ olur ustteki eksi isaretinin sebebi bu. 

Peki $\sum_{d=1}^{D}$ nereye gitti? Turevi $\alpha_d$'ye gore aliyoruz ve o
turevi alirken tek bir $\alpha_d$ ile ilgileniyoruz, mesela $\alpha_{22}$,
bunun haricindeki diger tum $\alpha_?$ degerleri turev alma islemi
sirasinda sabit kabul edilirler, turev sirasinda sifirlanirlar. Bu sebeple
$\sum_{d=1}^{D}$ icinde sadece bizim ilgilendigimiz $\alpha_d$ geriye
kalir. Tabii ki bu ayni zamanda her $d=1,2,..D$, $\alpha_d$ icin ayri bir
turev var demektir, ama bu turevlerin hepsi birbirine benzerler, yani tek
bir $\alpha_d$'yi cozmek, hepsini cozmek anlamina gelir.

Devam edelim:

\[ \sum_{i=1}^N \bigg( \frac{x_d^i}{\alpha_d} - \frac{1-x_d^i}{1-\alpha_d} \bigg) =
\frac{N_d}{\alpha_d} - \frac{N-N_d}{1-\alpha_d} = 0
 \]

$\sum_{i=1}^N x_d^i = N_d$ olarak kabul ediyoruz, $N_d$ tum veri icinde $d$
boyutu (kelimesi) '1' kac tane hucre oldugunu bize soyler. $x_d^i$ ya '1' ya '0'
olabildigine gore bir $d$ icin, tum $N$ hucrenin toplami otomatik olarak bize
kac tane '1' oldugunu soyler. Sonra:

\[ \frac{N_d}{\alpha_d} - \frac{N-N_d}{1-\alpha_d} = 0  \]

\[ \frac{1-\alpha_d}{\alpha_d} = \frac{N-N_d}{N_d}   \]

\[ \frac{1}{\alpha_d} - 1 = \frac{N}{N_d} - 1  \]

\[ \frac{1}{\alpha_d} = \frac{N}{N_d}  \]

\[ \alpha_d = \frac{N_d}{N}  \]

Python Kodu

$\alpha_d$'nin formulunu buldumuza gore artik kodu yazabiliriz. Ilk once bir
dokumani temsil eden cok boyutlu Bernoulli vektorunu ortaya cikartmamiz
lazim. Bu vektorun her hucresi belli bir kelime olacak, ve o kelimelerin ne
oldugunu onceden kararlastirmamiz lazim. Bunun icin her siniftaki tum
dokumanlardaki tum kelimeleri iceren bir sozluk yaratiriz:

\begin{lstlisting}[language=Python]
words = {}

# find all words in all files, creating a 
# global dictionary.
for file in ['a1.txt','a2.txt','a3.txt',
             'b1.txt','b2.txt','b3.txt']:
    f = open (file)
    s = f.read()
    tokens = re.split('\W+', s)
    for x in tokens: words[x] = 0.
\end{lstlisting}

Dosyalar 'a1.txt','a2.txt','a3.txt' Hawking kitabindan sayfalar,
'b1.txt','b2.txt','b3.txt' ise Obama sayfalari. Ana sozlugu yarattigimiza gore
artik her yazar icin ayri egitim yapabiliriz. Hawking icin 

\begin{lstlisting}[language=Python]
hawking_alphas = words.copy()   
for file in ['a1.txt','a2.txt','a3.txt']:
    words_hawking = set()
    f = open (file)
    s = f.read()
    tokens = re.split('\W+', s)
    for x in tokens: 
        words_hawking.add(x)
    for x in words_hawking:
        hawking_alphas[x] += 1.        
\end{lstlisting}

Niye set() kullandik? Bilindigi gibi set() objesi bir kume demektir, kume
mantigina gore bir eleman kumede var ise, onu bir daha eklemek kume icerigini
degistirmez. Bu tam bize gore, cunku tek bir belgeye bakarken, bir kelime ``en
az bir kere'' var ise o kelimenin hucresi '1' olacak, tek belge icinde kelimenin
kac kere cikacagi bizi ilgilendirmiyor. Sonra bu kumeye bakiyoruz ve kumedeki
her kelimeye tekabul eden hucreyi \verb!hawking_alphas[x] += 1.! ile '1'
arttiriyoruz. Obama sayfalari icin yapilan islem ayni. Sonra her iki alpha
vektorunu toplam dokuman sayisina (3) boluyoruz cunku $\alpha_d = N_d / N$.

\begin{lstlisting}[language=Python]
for x in hawking_alphas.keys():
    hawking_alphas[x] = hawking_alphas[x] / 3.        
for x in obama_alphas.keys():
    obama_alphas[x] = obama_alphas[x] / 3.        
\end{lstlisting}

Bu noktada hic gorulmemis yeni bir dokumani siniflamaya haziriz. Diyelim ki
a4.txt adli bir dosyamiz var, biz bunun Hawking'in kitabindan bir sayfa oldugunu
biliyoruz (ama program bilmiyor) ve \verb!test('a4.txt')! cagrisini yaparak dogru
sonucu almayi umuyoruz. 

\begin{lstlisting}[language=Python]
def prob(xd, alpha):
    return math.log(alpha*xd + 1e-10) + \
        math.log((1.-alpha)*(1.-xd) + 1e-10)
        
def test(file):
    test_vector = words.copy()   
    words_test = set()
    f = open (file)
    s = f.read()
    tokens = re.split('\W+', s)
    for x in tokens: 
        words_test.add(x)
    for x in words_test:  
        test_vector[x] = 1.
    ob = 0.
    ha = 0.
    for x in test_vector.keys(): 
        if x in obama_alphas: 
           ob += prob(test_vector[x], obama_alphas[x])
        if x in hawking_alphas: 
           ha += prob(test_vector[x], hawking_alphas[x])
                
    print "obama", ob, "hawking", ha, "obama", ..

print "hawking test"    
test('a4.txt')
\end{lstlisting}

Test icin yeni dokumani kelimelerine ayiriyoruz, ve her kelimeye tekabul eden
alpha vektorlerini kullanarak bir yazar icin toplam olasiligi
hesapliyoruz. Nasil? Her kelimeyi $\alpha_d^{x_d}(1-\alpha_d)^{1-x_d}$ formulune
soruyoruz, yeni dokumani temsilen elimizde bir $[1,0,0,1,0,0,...,1]$ seklinde
bir vektor oldugunu farz ediyoruz, buna gore mesela $x_1=1$, $x_2=0$. Eger bir
$d$ kelimesi yeni belgede ``var'' ise o kelime icin $x_d = 1$ ve bu durumda
$\alpha_d^{x_d} = \alpha_d^{1} = \alpha_d$ haline gelir, ama formulun oteki
tarafi yokolur, $(1-\alpha_d)^{1-x_d} = (1-\alpha_d)^0 = 1$, o zaman $\alpha_d
\cdot 1 = \alpha_d$.

Carpim diyoruz ama biz aslinda siniflama sirasinda
$\alpha_d^{x_d}(1-\alpha_d)^{1-x_d}$ carpimi yerine yine log() numarasini
kullandik; cunku olasilik degerleri hep 1'e esit ya da ondan kucuk sayilardir,
ve bu kucuk degerlerin birbiriyle surekli carpimi nihai sonucu asiri fazla
kucultur. Asiri ufak degerlerle ugrasmamak icin olasiliklarin log'unu alip
birbirleri ile toplamayi sectik, yani hesapladigimiz deger $x_d \cdot
log(\alpha_d) + (1-x_d) \cdot log(1-\alpha_d)$

Fonksiyon \verb!prob! icindeki \verb!1e-7! kullanimi neden? Bu kullanim log
numarisini yapabilmek icin -- sifir degerinin log degeri tanimsizdir, bir
kelime olmadigi zaman log'a sifir gelecegi icin hata olmamasi icin log icindeki 
degerlere her seferinde yeterince kucuk bir sayi ekliyoruz, boylece pur sifirla 
ugrasmak zorunda kalmiyoruz. Sifir olmadigi zamanlarda cok eklenen cok kucuk bir
sayi sonucta buyuk farklar (hatalar) yaratmiyor.

Toparlarsak, yeni belge 'a4.txt' icin iki tur alpha degerleri kullanarak iki
farkli log toplamini hesaplatiyoruz. Bu iki toplami birbiri ile karsilastiriyoruz,
hangi toplam daha buyukse, dokumanin o yazardan gelmesi daha olasidir, ve o
secimimiz o yazar olur.

Kodun geri kalani \verb!naive.py! dosyasinda bulunabilir, test cagrilarina
bakilirsa, yeni test dokumanlarinin dogru sekilde siniflandigi gorulecektir.

\lstinputlisting[language=Python]{naive.py}

Kaynaklar

Jebara, T., Columbia U., {\em COMS 4771 Machine Learning} Lecture Notes, Lecture
7

\end{document}
