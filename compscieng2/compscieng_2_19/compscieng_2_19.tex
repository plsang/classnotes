\documentclass[12pt,fleqn]{article}\usepackage{../common}
\begin{document}
Ders 19

Eslenik Gradyan (Conjugate Gradient) Yontemi 

Arnoldi metotu Gram-Schmidt'e benzeyen bir yontemdir ve bir ortogonal baz
ortaya cikartir. Bu baz, Krylov altuzayinin bazidir, ki bu altuzaydaki her
yeni baz vektor, $e$'nin baska bir ustu alinip carpilarak elde
edilir. Fakat bu pek iyi bir baz degildir, bazlarin ortogonalize edilmesi
gerekir, ve Arnoldi'nin yaptigi budur.

Arnoldi-Lanczos yontemi ozdegerler (eigenvalue) bulmak icin de kullanilir.

\[ AQ = QH \]

esitligindeki $H$ matrisinin alt-matrisine bakilirsa, aranilan ozdegerler
buradan okunabilir. Bu alt-matris simetrik ve ust kosegendir.
(upperdiagonal). 

\[ H = Q^{-1}AQ \]

formulunde $H,A$ matrisleri birbirine benzerdir (similar) ve benzer
matrislerin ozdegerleri aynidir. 

Bu kavramlardan soyle bir bahsetmek istedim, belki gunun birinde cok buyuk
bir matrisin ozdegerlerini bulmak istersiniz, aklinizda olsun. Yazilim
\verb!arpack! bunun icin kullanilabiliyor. Bahsi yaptik bir diger sebep
lineer cebirin yarisi lineer sistemlerse, diger yarisi ozdeger
problemleridir denebilir. Buraya gelmisken ustteki ozdeger yonteminden
bahsetmemek olmazdi. 

Konumuza donelim. 

$A$ pozitif kesin ve simetrik olmali. Eger degilse birazdan gosgterecegimiz
formulleri kullanmak biraz riskli olur, isleyebilirler ama garanti olmaz. 

$r_K = b - Ax_k $, $K_k$'ye ortogonal, $x_k \in \mathscr{K}_K$. 

Demek ki $x_k$'yi ozyineli olarak yaratabiliriz, ve her adimda sadece $A$
ile carpmamiz gerekir. Ustteki formulde $A$ ile carpim olduguna gore, $r_K$
bir sonraki uzay $k+1$ icinde olacaktir. Arnoldi'den biliyoruz ki $q_{k+1}$
ayni uzay icindedir. O zaman 

$r_k$, $q_{k+1}$'in bir katidir. Yani $r$ ile gosterilen ``artiklar
(residuals)'' birbirine ortogonal. Yani 

\[ r_i^Tr_k = 0, \ i < k \]

Artiklarin birbirine ortogonal olmasinin sebebi iclerinde $A$ olmasi. 

Baslangic degerleri

\[ d_0 = b \]

\[ x_0 = 0 \]

\[ r_0 = b - Ax_0  = b\]

Simetrik Pozitif Kesin $A$ Icin Eslenik Gradyan Metodu

\begin{lstlisting}[language=Matlab,mathescape,numbers=left,xleftmargin=3.0ex]
$\alpha_k = r_{k-1}^T r_{k-1} / d_{k-1}^T A d_{k-1}$
$\alpha_k = \alpha_{k-1} + \alpha_k d_{k-1}$
$r_k = r_{k-1} - \alpha_k Ad_{k-1}$
$\beta_k = r_k^Tr_k / r_{k-1}^Tr_{k-1}$
$d_k = r_k + \beta_k d_{k-1}$
\end{lstlisting}

$d$ ``arama yonudur'', optimizasyon ilerlerken gidecegimiz
istikamettir. 2. adimda guncellemeyi yapiyorum. Peki bir sonraki yonum ne
olmali? 

Her Dongude:

- $Ad$ carpimini goruyoruz, cunku $A$ ile carpim bize yeni Krylov altuzayini
veriyor.\\
- 2 icsel carpim \\
- 2 ya da 3 vektor guncellemesi

Peki $k$ adim sonra hata $||e_k||$ nedir ve ilk bastaki hata $||e_0||$ ile
baglantisi nedir? 

\[ ||e_k|| \le 2  \bigg(
\frac{ \sqrt{ \lambda_{maks} - \lambda_{min}}}
{\lambda_{maks} + \lambda_{min}}
\bigg)^k||e_0||
\]

Hala bir kelimeye aciklik getirmedik; gradyan. Niye bir ``gradyan''
kelimesi kullaniyoruz, neyin gradyanindan bahsediyoruz, bu teknik icin
gradyanlar ne anlama geliyor?

Lineer problemlerde $Ax = b$ esitligi vardir ve bu esitlik enerjinin
gradyanindan gelir. Yani 

\[ E(x) = \frac{ 1}{2}x^TAx - b^Tx \]

enerjisinin gradyanindan. Ustteki formul nereden geldik diye
dusunebilirsiniz, hep lineer sistemlerden bahsettik, ve bu sistemlerde her
sey $Ax = b$ formatina uyar. Simdi birdenbire matematigin farkli bir koluna
geciyorum sanki, ustteki formulu minimize etmeye ugrasiyorum, yani
optimizasyona giriyorum. Fakat cebirsel olarak dusunursek, 

\[ grad \ E = [\frac{\partial E}{\partial x} ]  = Ax - b \]

olacaktir. Minimumda ustteki sifir olacagina gore 

\[ Ax - b = 0 \]

\[ Ax = b \]

Yani karesel enerjinin lineer gradyani vardir, ve onun minimumu $Ax =
b$'dir. 
Bu demektir ki lineer denklemi cozmek ve enerjiyi minimize etmek
aslinda ayni seydir! Minimum kelimesini kullanabiliyorum bu arada, cunku
$A$'nin pozitif kesin oldugunu biliyorum. 

Minimize islemi nasil yapilir? Diyelim ki alttaki gibi bir $E(x)$'im var,
kap seklinin herhangi bir noktasindayim, ve asagi inmem lazim. En fazla
artis gradyan $g$ ise, dibe inmek icin $-g$ yonunde gidebilirim. 

\includegraphics[height=2cm]{19_1.png}

Bu yon dogal bir yondur, ilk akla gelen fikirdir ve mantiklidir. Fakat en
iyi yon degildir. Simdi minimizasyon cozumu olarak eslenik gradyan
acisindan bakiyoruz olaya, isin gradyan tarafi da boylelikle acikliga
kavusacak. 

Negatif gradyanin ayni zamanda artigin da (residual) negatif
yonudur. Artigin yonunde hareket etmek iyi midir? Negatif gradyani takip
etmenin bir diger ismi ``en dik inis (steepest descent)''tir. Fakat,
baslangic noktasina gore bu degisir ama, cok fazla inis cikis ta
yasanabilir.

$r$'ler hesapsal bilimde cok aranan bir ozellige sahip degildir,
ortogonallik. Bir sekilde ortogonallik her zaman dogru yonde hareket
ettigimizin garantisidir. Gidilmesi gereken dogru yon, ustteki kodda
5. satirda hesaplanan yondur. Bu yone ``$A$-ortogonal'' denir. 

Bir resimle gostermek gerekirse, alta bakalim, soldaki en dik inis, sagdaki
eslenik gradyan. Enerji fonksiyonunu kesit seviyesinden (level set), cevrit
(contour) olarak gosteriyoruz, her cevrit bir enerji seviyesine tekabul
edecek, mesela en distaki cevrit 5, bir icerideki 4 olabilir, ve en
ortadaki nokta tam sifir olabilir, cunku en dusuktur. 

\includegraphics[height=4cm]{19_2.png}

Her iki teknigin gidisati resimde gorulmektedir. 

[gerisi atlandi]

Ekler 

Ustteki anlatimda Krylov altuzaylarinin eslenik gradyan metotunun
isleyisinde tam olarak nasil rol oynadigi belirtilmemis. Aslinda Krylov
altuzaylari gerektirmeden bu metotu anlatmak mumkun. 

Iki vektor $u,v$ birbirine A-ortogonaldir eger

\[ u^TAv = 0 \] 

ise. Dikkat, bu iki vektor, tek baslarina, $u^Tv$ olarak birbirine
ortogonal olmayabilir, ama ortada $A$ oldugu halde carpim sifir cikarsa
ortogonal olmasalar da A-ortogonal olurlar. Bu ortogonalligin bir diger
ismi eslenik (conjugate) olmaktir.

Simdi diyelim ki elimizde herbiri birbirine ortogonal olan $n$ tane
$\{p_k\}$ yonu / vektoru var. O zaman $p_k$ $\mathbb{R}^n$ icin bir baz
olusturur ve biz de $Ax = b$ denkleminin cozumu $x_*$'i bu bazi temel
alarak temsil ederiz. Yani baz vektorlerini carpan bazi katsayilar vardir,
ve bu carpimlarin toplami $x_*$ olur. 

\[ x_* = \sum _{ i=1}^{n} \alpha_i p_i \]

Boylece $Ax = b$'yi cozmek icin bir metot elde ediyoruz, eger $n$ tane
eslenik yon bulabilirsek, $\alpha$ degerlerini hemen hesaplayabiliriz.
Ayrica eger eslenik vektorler $p_k$'leri dikkatlice secersek, yaklasik cozum $x_*$
icin hepsine ihtiyacimiz olmaz. Ozyineli $x$ formulunu kullanabiliriz,

\[ x_{k+1} = x_k + \alpha_k p_{k+1} \ \ \ \label{1} \]

Bu formul niye mantikli? Eger cozum $x_*$ ortogonal $p_k$ vektorlerinin bir
lineer kombinasyonu ise, cozum vektorleri birbiri ardina dizilmis ve ``bir
yere giden'' bir zincir olarak gorulebilir. Ustteki formul sadece bu
zinciri yavas yavas kurmakta..

Ilk once ozyineli olarak artiklar $r_k$ arasinda bir iliski kuralim,
(1)'nin iki tarafi $A$ ile carpip, $-b$ cikartalim (cunku 
$r_i = b -
Ax_i$'a erismek istiyoruz),

\[A x_{k+1} - b = A x_k - b + \alpha_k A p_{k} \]

\[-r_{k+1} = -r_k + \alpha_k A p_{k} \]

\[r_{k+1} = r_k - \alpha_k A p_{k} \]

Simdi hata terimini hesaplayalim. $e_i$, yani $i$'inci tahminin hatasi, 

\[ e_i = x - x_i  \]

Iki tarafi $A$ ile carpalim

\[ Ae_i = Ax - Ax_i  \]

\[ Ae_i = b - Ax_i  \]

Sag taraf $r_i$ taniminin aynisi degil mi? O zaman 

\[ Ae_i = r_i 
\ \ \ \label{5}
\]

$e$'yi ozyineli olarak temsil etmek te mumkundur, (1)'nin her iki
tarafindan $x$ cikartirsam, 

\[ x_{k+1} - x = x_k - x + \alpha_k p_{k} \]

\[ e_{k+1} = e_k + \alpha_k p_k \ \ \ \label{2}\]

Bu her adimi $\alpha_k$'ye bagli ozyineli bir tanimdir. 

$\alpha$ katsayilarini bulmak icin bir sonraki yonden gelen hatanin, onceki
arama yonlerine, ya da bir onceki arama yonune A-ortogonal olmasini
istiyoruz. Yani

\[ p_k^TA e_{i+1}  = 0\]

olmali. 

\[ p_k^TA (x_{i+1}-x)  = 0\]

\[ p_k^TA (x_{i} + \alpha_i p_i -x)  = 0\]

\[ p_k^TA (e_i + \alpha_i p_i )  = 0\]

\[ p_k^Tr_i + \alpha_i A p_i   = 0\]

\[ \alpha_i = -\frac{ p_k^Tr_i}{\alpha_i A p_i} 
\ \ \ \label{6}
\]

Simdi hata terimine donelim, diyelim ki $e_0$ vektoru, bu vektor, diger
her vektor gibi icinde oldugumuz uzayin bazlarinin bir kombinasyonu olarak
temsil edilebilir. Bizim bazlarimiz $p_k$ olduguna gore, 

\[ e_0 = \sum _{ j=0}^{n-1} \delta_j p_j \]

Katsayi olarak $\delta_j$ sectik, $\alpha$ ile karisiklik olmasin
diye. Simdi iki tarafi $p_k^T A$ ile carpalim, 

\[ p_k^T A e_0 = \sum _{ j=0}^{n-1} \delta_j p_k^T A p_j \]

Yine ayni ortogonallik numarasi, toplam icinde $j$ olmayan tum diger $p$
carpimlari sifirdir, 

\[ p_k^T A e_0 =  \delta_j p_j^T A p_j \]

\[ \delta_j = \frac{ p_k^T A e_0}{ p_j^T A p_j } 
\ \ \ \label{4}
\]

Simdi $e_0$'in yerine (2)'teki ozyineli tanimdan turetecegim bir sey koymak
istiyorum. Diyelim ki $e_0$'dan baslayip teker teker bir sonraki $e$'yi
hesaplayip alt alta yazdim, ve topladim

\[ \cancel{e_{1}} = e_0 + \alpha_0 p_0\]

\[ \cancel{e_{2}} = \cancel{e_1} + \alpha_1 p_1\]

\[ ... \]

\[ e_{k} = \cancel{e_{k-1}} + \alpha_{k-1} p_{k-1}\]


Sag kalan tek terimler 

\[ e_k = e_0 + \sum _{ j=0}^{k-1} \alpha_j p_j \]

(4) icinde $e_0$ yerine koyalim

\[ \delta_j = \frac{ p_k^T A e_k - \cancel{\sum _{ j=0}^{k-1} \alpha_j p_k^T A p_j}}
{ p_j^T A p_j } 
\]

Niye iptal? Yine A-ortogonalligi. Dikkat edilirse $j$'ler $k-1$'e kadar
cikiyor, $k$'ye bile erismiyor, carpim hep sifir. Kalanlar,

\[  = \frac{ p_k^T A e_k}
{ p_j^T A p_j } 
\]

(5)'i kullanirsak, 

\[ \delta_j = \frac{ p_k^T r_k}
{ p_j^T A p_j } 
\]

(6) ile bu formulun benzerligi bariz, sadece eksi isareti farkli. O zaman 

\[ \delta_k = -\alpha_k \]

diyebiliriz. Bu demektir ki hata formulunde $\alpha$ yerine $\delta$
kullanabiliriz, 

\[ e_0 = -\sum _{ j=0}^{n-1} \alpha_j p_j \]

Hatalarin ozyineli denklemi (2)'yi uste uygularsak, 

\[ e_i = -\sum _{ j=i}^{n-1} \alpha_j p_j 
\ \ \ \label{3}
\]

Simdi artiklarin ve gidis yonlerinin ortogonal olduklarini
gosterelim. (3)'u $p_k^TA$ ile carpalim, 

\[ p_k^TAe_i = -\sum _{ j=i}^{n-1} \alpha_j  p_k^TAp_j 
\]

\[ p_k^Tr_i = -\sum _{ j=i}^{n-1} \alpha_j  p_k^TAp_j 
\]

$p$'ler arasindaki A-ortogonallik sayesinde ve $k < i$ icin 

\[ p_k^Tr_i = 0\]

Madem ki yonler ve artiklar birbirine ortogonal, Gram-Schmidt isleminin
A-ortogonal halini artiklardan yon uretmek icin kullanabiliriz. Her artigi
alip, icinden ona ortogonal bir yon cikartak mumkun. 



\lstinputlisting[language=Python]{cg.py}


\end{document}
