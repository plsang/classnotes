\documentclass[12pt,fleqn]{article}\usepackage{../common}
\begin{document}

\begin{minted}[fontsize=\footnotesize]{python}
occup_map = \
{ 0:  "other" or not specified,1:  "academic/educator",
  2:  "artist",3:  "clerical/admin",
  4:  "college/grad student",5:  "customer service",
  6:  "doctor/health care",7:  "executive/managerial",
  8:  "farmer",9:  "homemaker",
  10:  "K-12 student", 11:  "lawyer",
  12:  "programmer",13:  "retired",
  14:  "sales/marketing",15:  "scientist",
  16:  "self-employed",17:  "technician/engineer",
  18:  "tradesman/craftsman",19:  "unemployed",
  20:  "writer"}

zip_map = \
{ 0: 'Northeast', 1: 'NY Area', 2: 'DC', 3: 'Florida', 4: 'Michigan/Ohio', 
  5: 'North', 6: 'Illinois', 7: 'Texas / Arkansas', 8: 'Nevada / Utah', 
  9: 'California / Alaska'}
\end{minted}


\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
cols = ['user_id', 'gender', 'age', 'occupation', 'zip']
users = pd.read_csv('../stat_pandas_ratings/users.dat', sep='::', 
        header=None,names=cols)

from sklearn.feature_extraction import DictVectorizer
def one_hot_dataframe(data, cols):
    vec = DictVectorizer()
    mkdict = lambda row: dict((col, row[col]) for col in cols)
    tmp = vec.fit_transform(data[cols].to_dict(outtype='records')).toarray()
    vecData = pd.DataFrame(tmp)
    vecData.columns = vec.get_feature_names()
    vecData.index = data.index
    data = data.drop(cols, axis=1)
    data = data.join(vecData)
    return data

df = users.copy()
df['occupation'] = df.apply(lambda x: occup_map[x['occupation']], axis=1)
df['zip2'] = users['zip'].map(lambda x: int(str(x)[0]))
df['zip2'] = df.apply(lambda x: zip_map[x['zip2']], axis=1)
df = one_hot_dataframe(df,['occupation','gender','zip2'])
df = df.drop(['zip'],axis=1)
df = df.set_index('user_id')
\end{minted}



\begin{minted}[fontsize=\footnotesize]{python}
cols = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings =  pd.read_csv('../stat_pandas_ratings/ratings.dat', sep='::',
           header=None,names=cols)
cols = ['movie_id', 'title', 'genres']
movies =  pd.read_csv('../stat_pandas_ratings/movies.dat',sep='::',
          header=None,names=cols)

genre_iter = (set(x.split('|')) for x in movies.genres)
genres = sorted(set.union(*genre_iter))
dummies = pd.DataFrame(np.zeros((len(movies), len(genres))), columns=genres)
for i, gen in enumerate(movies.genres):
   dummies.ix[i, gen.split('|')] = 1
movies_windic = movies.join(dummies.add_prefix('Genre_'))
movies_windic = movies_windic.drop(['title','genres'],axis=1)
joined = ratings.merge(movies_windic, left_on='movie_id',right_on='movie_id')
genres = joined.groupby('user_id').sum()
genres = genres.drop(['movie_id','rating','timestamp'],axis=1)
X = pd.merge(df, genres, left_index=True, right_index=True,how='left')
print X.shape
\end{minted}

\begin{verbatim}
(6040, 52)
\end{verbatim}
\begin{minted}[fontsize=\footnotesize]{python}
fout = open('/tmp/featmap.txt','wb')
for i,col in enumerate(X.columns):
    if  'age' in col: fout.write('%d\t%s\tq\n' % (i,col))
    else: fout.write('%d\t%s\ti\n' % (i,col))    
fout.close()
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
print list(X.columns)
from sklearn.preprocessing import normalize
import scipy.sparse.linalg as slin
import scipy.sparse as sps, numpy.random as rand
X2 = sps.csr_matrix(X.fillna(0))
X2 = normalize(X2, norm='l2', axis=0).tocsr()
X2 = normalize(X2, norm='l2', axis=1).tocsr()    
rand.seed(1000)
Omega = sps.csr_matrix(rand.randn(X2.shape[1],4))
u = X2.dot(Omega)
\end{minted}

\begin{verbatim}
['age', 'gender=F', 'gender=M', 'occupation2=0', 'occupation2=1', 'occupation2=10', 'occupation2=11', 'occupation2=12', 'occupation2=13', 'occupation2=14', 'occupation2=15', 'occupation2=16', 'occupation2=17', 'occupation2=18', 'occupation2=19', 'occupation2=2', 'occupation2=20', 'occupation2=3', 'occupation2=4', 'occupation2=5', 'occupation2=6', 'occupation2=7', 'occupation2=8', 'occupation2=9', 'zip2=0', 'zip2=1', 'zip2=2', 'zip2=3', 'zip2=4', 'zip2=5', 'zip2=6', 'zip2=7', 'zip2=8', 'zip2=9', 'Genre_Action', 'Genre_Adventure', 'Genre_Animation', "Genre_Children's", 'Genre_Comedy', 'Genre_Crime', 'Genre_Documentary', 'Genre_Drama', 'Genre_Fantasy', 'Genre_Film-Noir', 'Genre_Horror', 'Genre_Musical', 'Genre_Mystery', 'Genre_Romance', 'Genre_Sci-Fi', 'Genre_Thriller', 'Genre_War', 'Genre_Western']
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
n_clusters=30
from sklearn.cluster import KMeans
print u.shape
clf = KMeans(n_clusters=n_clusters)
clf.fit(u)    
df2 = X.reset_index()
df2['cluster'] = clf.labels_
df2.to_csv('/tmp/customers_clustered.csv',sep=';',index=None)
\end{minted}

\begin{verbatim}
(6040, 4)
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.metrics import roc_curve, auc
from sklearn.cross_validation import train_test_split
import os.path,os,logging,datetime,pytz,re, sys
sys.path.append('%s/Downloads/xgboost/wrapper/' % os.environ['HOME'])
import xgboost as xgb

df3 = pd.read_csv('/tmp/customers_clustered.csv',sep=';',index_col='user_id')
X3 = df3.drop('cluster',axis=1)
print X3.shape

aucs = []
for i in range(n_clusters):
    y = (df3['cluster'] == i).astype(float)
    X_train, X_test, y_train, y_test = train_test_split(X3, y, test_size=0.05, random_state=0)
    xg_train = xgb.DMatrix(X_train, label=y_train)
    xg_test = xgb.DMatrix(X_test, label=y_test)    
    watchlist = [ (xg_train,'train'), (xg_test, 'test') ]    
    param = {}; 
    num_round = 1
    param['silent'] = 1
    param['eval_metric'] = 'auc'
    param['max_depth'] = 4
    bst = xgb.train(param, xg_train, num_round, watchlist )
    pred = bst.predict(xg_test)
    fpr, tpr, thresholds = roc_curve(y_test, pred)
    roc_auc = auc(fpr, tpr)
    print 'cluster', i, roc_auc, np.sum(y)
    aucs.append(roc_auc)
    bst.dump_model('/tmp/tree-%d.txt' % i,'/tmp/featmap.txt')
print 'mean auc', np.array(aucs).mean()
\end{minted}

\begin{verbatim}
(6040, 52)
cluster 0 1.0 136.0
cluster 1 0.804255319149 382.0
cluster 2 0.980046948357 223.0
cluster 3 0.784997002398 347.0
cluster 4 0.889225589226 129.0
cluster 5 1.0 124.0
cluster 6 0.826342281879 191.0
cluster 7 0.989726027397 172.0
cluster 8 0.923287671233 174.0
cluster 9 0.991041162228 104.0
cluster 10 0.979332574896 156.0
cluster 11 0.997747747748 78.0
cluster 12 0.794612794613 137.0
cluster 13 0.825423728814 195.0
cluster 14 0.911781609195 263.0
cluster 15 0.888208677136 181.0
cluster 16 0.977907905244 245.0
cluster 17 0.827504960317 298.0
cluster 18 1.0 199.0
cluster 19 0.973644292757 211.0
cluster 20 0.930992736077 173.0
cluster 21 0.941347270616 222.0
cluster 22 0.997661564626 155.0
cluster 23 0.836643835616 187.0
cluster 24 0.990427927928 169.0
cluster 25 0.952806122449 200.0
cluster 26 0.972390572391 163.0
cluster 27 0.865389082462 253.0
cluster 28 0.987490018632 238.0
cluster 29 0.91867816092 335.0
mean auc 0.925297119477
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
import pickle
all_rules = {}

# Starting from node follow parents all the way to the top to
# determine the "path" that led to that node
def trace(node):
    tr = []
    while node:
        # we start from bottom so dont append, insert in the
        # beginning for the right trace
        tmp = node # if name exists use it
        if tmp in nodes: tmp=nodes.get(node)
        tr.insert(0, (tmp,nodes_yn.get(node)))
        node=parents.get(node)
    return tr

# Walk the tree using recursion, visiting each node, until
# reaching a bottommost leaf, and report the path from root to
# that bottommost leaf. Those paths are our rules.
def walk(T,node):
    if node not in branches:
        rules.append(trace(node))
        return
    left, right = branches[node]
    parents[left] = node
    walk(T, left)
    parents[right] = node
    walk(T, right)

for cluster in range(n_clusters):
    #print 'Tree %d' % cluster
    tree = open('/tmp/tree-%d.txt' % cluster).read()

    nodes_yn = {}
    nodes = re.findall(r'(\d+):\[(.*?)\]',tree,re.DOTALL|re.MULTILINE)
    nodes = dict(x for x in nodes)
    branches = re.findall(r'(\d+):\[.*?(yes=\d+),(no=\d+)',tree,re.DOTALL|re.MULTILINE)
    for x in branches: nodes_yn[x[1].replace('yes=','')] = 'yes'
    for x in branches: nodes_yn[x[2].replace('no=','')] = 'no'
    branches = dict((x[0],(x[1].replace('yes=',''),x[2].replace('no=',''))) for x in branches)
    leaves = re.findall(r'(\d+):leaf=(.*?)\n',tree,re.DOTALL|re.MULTILINE)        
    leaves = dict(x for x in leaves)

    parents = {}; rules = []


    walk(nodes, '0')

    # filter out rules whose bottommost leaf has probability has 0
    #rules = [rule for rule in rules if float( leaves[rule[-1][0]] ) > 0.0]
    new_rules = []
    for rule in rules:
        tmp = [(rule[i][0],rule[i+1][1]) for i in range(len(rule)-1)]
        new_rules.append(tmp)
    all_rules[cluster] = new_rules
    
df = pd.read_csv('/tmp/customers_clustered.csv',sep=';',index_col='user_id')
print 'original dataset', len(df)

rules = all_rules
j = 0
for i in range(n_clusters):
    for r,rule in enumerate(rules[i]): 
        df_slice = df.copy()
        for x in rule: 
            if '<' not in x[0]: 
                df_slice = df_slice[df_slice[x[0]] == int(x[1]=='yes')]
            else:
                a,b = x[0].split('<')
                df_slice = df_slice[(df_slice[a] < float(b)) == (x[1]=='yes') ]

        # only report if slice count is high
        if len(df_slice) > 400:
            print 'Rule', j
            print
            for x in rule: print x
            print
            print 'count', len(df_slice)
            print
            j += 1
\end{minted}

\begin{verbatim}
original dataset 6040
Rule 0

('zip2=0', 'no')

count 5378

Rule 1

('zip2=9', 'yes')
('occupation2=0', 'no')
('occupation2=16', 'no')
('occupation2=12', 'no')

count 1095

Rule 2

('zip2=9', 'no')
('occupation2=6', 'no')
('zip2=2', 'yes')
('occupation2=14', 'no')

count 409

Rule 3

('zip2=9', 'no')
('occupation2=6', 'no')
('zip2=2', 'no')
('occupation2=16', 'no')

count 3803

Rule 4

('occupation2=15', 'no')
('occupation2=17', 'no')

count 5394

Rule 5

('occupation2=11', 'no')
('zip2=4', 'yes')
('occupation2=7', 'no')
('occupation2=12', 'no')

count 518

Rule 6

('occupation2=11', 'no')
('zip2=4', 'no')
('Genre_Animation', 'yes')
('zip2=7', 'no')

count 718

Rule 7

('occupation2=11', 'no')
('zip2=4', 'no')
('Genre_Animation', 'no')

count 1091

Rule 8

('zip2=1', 'yes')
('occupation2=1', 'no')
('occupation2=0', 'no')
('occupation2=16', 'no')

count 495

Rule 9

('zip2=1', 'no')

count 5378

Rule 10

('zip2=5', 'yes')
('occupation2=4', 'no')
('occupation2=0', 'no')
('occupation2=16', 'no')

count 484

Rule 11

('zip2=5', 'no')
('zip2=6', 'no')
('occupation2=9', 'no')

count 4876

Rule 12

('zip2=5', 'yes')
('occupation2=12', 'no')
('occupation2=6', 'no')
('occupation2=16', 'no')

count 556

Rule 13

('zip2=5', 'no')

count 5381

Rule 14

('zip2=5', 'yes')
('occupation2=1', 'no')
('occupation2=10', 'no')
('occupation2=20', 'no')

count 547

Rule 15

('zip2=5', 'no')
('zip2=2', 'no')
('occupation2=6', 'no')

count 4740

Rule 16

('occupation2=8', 'no')
('zip2=0', 'yes')
('occupation2=7', 'no')

count 572

Rule 17

('occupation2=8', 'no')
('zip2=0', 'no')

count 5363

Rule 18

('occupation2=9', 'no')
('occupation2=2', 'no')

count 5681

Rule 19

('zip2=4', 'no')
('occupation2=1', 'yes')
('zip2=8', 'no')
('zip2=6', 'no')

count 407

Rule 20

('zip2=4', 'no')
('occupation2=1', 'no')
('Genre_Horror', 'no')
('occupation2=3', 'no')

count 573

Rule 21

('occupation2=1', 'no')
('zip2=1', 'yes')
('occupation2=7', 'no')
('occupation2=12', 'no')

count 464

Rule 22

('occupation2=1', 'no')
('zip2=1', 'no')
('occupation2=20', 'no')

count 4665

Rule 23

('occupation2=14', 'no')
('occupation2=7', 'yes')
('zip2=8', 'no')
('zip2=6', 'no')

count 601

Rule 24

('occupation2=14', 'no')
('occupation2=7', 'no')
('occupation2=11', 'no')
('occupation2=12', 'no')

count 4542

Rule 25

('occupation2=5', 'no')
('occupation2=12', 'no')
('occupation2=6', 'no')
('zip2=2', 'no')

count 4904

Rule 26

('Genre_Fantasy', 'no')
('Genre_Fantasy', 'no')
('zip2=4', 'no')
('occupation2=3', 'no')

count 1039

Rule 27

('occupation2=13', 'no')
('zip2=0', 'yes')
('occupation2=0', 'no')
('occupation2=10', 'no')

count 546

Rule 28

('occupation2=2', 'no')
('occupation2=19', 'no')
('occupation2=17', 'yes')
('zip2=6', 'no')

count 473

Rule 29

('zip2=1', 'yes')
('occupation2=20', 'no')
('occupation2=10', 'no')
('occupation2=7', 'no')

count 497

Rule 30

('zip2=1', 'no')
('occupation2=1', 'no')
('occupation2=17', 'yes')
('zip2=4', 'no')

count 421

Rule 31

('zip2=1', 'no')
('occupation2=1', 'no')
('occupation2=17', 'no')

count 4439

Rule 32

('occupation2=17', 'no')
('occupation2=19', 'no')
('occupation2=15', 'no')
('occupation2=13', 'no')

count 5180

Rule 33

('occupation2=7', 'no')
('occupation2=11', 'no')

count 5232

Rule 34

('occupation2=3', 'no')
('zip2=4', 'no')
('Genre_Documentary', 'no')

count 3289

Rule 35

('occupation2=18', 'no')
('occupation2=2', 'no')
('occupation2=17', 'yes')
('zip2=5', 'no')

count 449

Rule 36

('occupation2=18', 'no')
('occupation2=2', 'no')
('occupation2=17', 'no')
('occupation2=15', 'no')

count 5057

Rule 37

('occupation2=6', 'no')
('occupation2=5', 'no')
('zip2=5', 'yes')
('occupation2=7', 'no')

count 566

Rule 38

('occupation2=6', 'no')
('occupation2=5', 'no')
('zip2=5', 'no')
('occupation2=12', 'no')

count 4741

Rule 39

('zip2=6', 'no')
('zip2=8', 'no')
('zip2=5', 'no')
("Genre_Children's", 'yes')

count 616

Rule 40

('zip2=6', 'no')
('zip2=8', 'no')
('zip2=5', 'no')
("Genre_Children's", 'no')

count 609

Rule 41

('occupation2=20', 'no')
('occupation2=10', 'no')
('zip2=7', 'no')
('zip2=0', 'yes')

count 613

Rule 42

('occupation2=20', 'no')
('occupation2=10', 'no')
('zip2=7', 'no')
('zip2=0', 'no')

count 4569

\end{verbatim}





\url{http://upload.wikimedia.org/wikipedia/commons/2/24/ZIP_Code_zones.svg}

\url{http://www.zipboundary.com/zipcode_faqs.html}

\end{document}
