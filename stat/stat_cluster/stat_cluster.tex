\documentclass[12pt,fleqn]{article}\usepackage{../common}
\begin{document}

\begin{minted}[fontsize=\footnotesize]{python}
occup_map = \
{ 0:  "other" or not specified,1:  "academic/educator",
  2:  "artist",3:  "clerical/admin",
  4:  "college/grad student",5:  "customer service",
  6:  "doctor/health care",7:  "executive/managerial",
  8:  "farmer",9:  "homemaker",
  10:  "K-12 student", 11:  "lawyer",
  12:  "programmer",13:  "retired",
  14:  "sales/marketing",15:  "scientist",
  16:  "self-employed",17:  "technician/engineer",
  18:  "tradesman/craftsman",19:  "unemployed",
  20:  "writer"}

zip_map = \
{ 0: 'Northeast', 1: 'NY Area', 2: 'DC', 3: 'Florida', 4: 'Michigan/Ohio', 
  5: 'North', 6: 'Illinois', 7: 'Texas / Arkansas', 8: 'Nevada / Utah', 
  9: 'California / Alaska'}
\end{minted}


\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
cols = ['user_id', 'gender', 'age', 'occupation', 'zip']
users = pd.read_csv('../stat_pandas_ratings/users.dat', sep='::', 
        header=None,names=cols)

from sklearn.feature_extraction import DictVectorizer
def one_hot_dataframe(data, cols):
    vec = DictVectorizer()
    mkdict = lambda row: dict((col, row[col]) for col in cols)
    tmp = vec.fit_transform(data[cols].to_dict(outtype='records')).toarray()
    vecData = pd.DataFrame(tmp)
    vecData.columns = vec.get_feature_names()
    vecData.index = data.index
    data = data.drop(cols, axis=1)
    data = data.join(vecData)
    return data

df = users.copy()
df['occupation'] = df.apply(lambda x: occup_map[x['occupation']], axis=1)
df['zip2'] = users['zip'].map(lambda x: int(str(x)[0]))
df['zip2'] = df.apply(lambda x: zip_map[x['zip2']], axis=1)
df = one_hot_dataframe(df,['occupation','gender','zip2'])
df = df.drop(['zip'],axis=1)
df = df.set_index('user_id')
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
cols = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings =  pd.read_csv('../stat_pandas_ratings/ratings.dat', sep='::',
           header=None,names=cols)
cols = ['movie_id', 'title', 'genres']
movies =  pd.read_csv('../stat_pandas_ratings/movies.dat',sep='::',
          header=None,names=cols)

genre_iter = (set(x.split('|')) for x in movies.genres)
genres = sorted(set.union(*genre_iter))
dummies = pd.DataFrame(np.zeros((len(movies), len(genres))), columns=genres)
for i, gen in enumerate(movies.genres):
   dummies.ix[i, gen.split('|')] = 1
movies_windic = movies.join(dummies.add_prefix('Genre_'))
movies_windic = movies_windic.drop(['title','genres'],axis=1)
joined = ratings.merge(movies_windic, left_on='movie_id',right_on='movie_id')
genres = joined.groupby('user_id').sum()
genres = genres.drop(['movie_id','rating','timestamp'],axis=1)
X = pd.merge(df, genres, left_index=True, right_index=True,how='left')
print X.shape
\end{minted}

\begin{verbatim}
(6040, 52)
\end{verbatim}
\begin{minted}[fontsize=\footnotesize]{python}
fout = open('/tmp/featmap.txt','wb')
for i,col in enumerate(X.columns):
    if  'age'==col: fout.write('%d\t%s\tq\n' % (i,col))
    else: fout.write('%d\t%s\ti\n' % (i,col))    
fout.close()
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
print list(X.columns)
from sklearn.preprocessing import normalize
import scipy.sparse.linalg as slin
import scipy.sparse as sps, numpy.random as rand
X2 = sps.csr_matrix(X.fillna(0))
X2 = normalize(X2, norm='l2', axis=0).tocsr()
X2 = normalize(X2, norm='l2', axis=1).tocsr()    
rand.seed(1000)
Omega = sps.csr_matrix(rand.randn(X2.shape[1],4))
u = X2.dot(Omega)
\end{minted}

\begin{verbatim}
['age', 'gender=F', 'gender=M', 'occupation=K-12 student', 'occupation=academic/educator', 'occupation=artist', 'occupation=clerical/admin', 'occupation=college/grad student', 'occupation=customer service', 'occupation=doctor/health care', 'occupation=executive/managerial', 'occupation=farmer', 'occupation=homemaker', 'occupation=lawyer', 'occupation=other', 'occupation=programmer', 'occupation=retired', 'occupation=sales/marketing', 'occupation=scientist', 'occupation=self-employed', 'occupation=technician/engineer', 'occupation=tradesman/craftsman', 'occupation=unemployed', 'occupation=writer', 'zip2=California / Alaska', 'zip2=DC', 'zip2=Florida', 'zip2=Illinois', 'zip2=Michigan/Ohio', 'zip2=NY Area', 'zip2=Nevada / Utah', 'zip2=North', 'zip2=Northeast', 'zip2=Texas / Arkansas', 'Genre_Action', 'Genre_Adventure', 'Genre_Animation', "Genre_Children's", 'Genre_Comedy', 'Genre_Crime', 'Genre_Documentary', 'Genre_Drama', 'Genre_Fantasy', 'Genre_Film-Noir', 'Genre_Horror', 'Genre_Musical', 'Genre_Mystery', 'Genre_Romance', 'Genre_Sci-Fi', 'Genre_Thriller', 'Genre_War', 'Genre_Western']
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
n_clusters=20
from sklearn.cluster import KMeans
print u.shape
clf = KMeans(n_clusters=n_clusters)
clf.fit(u)    
df2 = X.reset_index()
df2['cluster'] = clf.labels_
df2.to_csv('/tmp/customers_clustered.csv',sep=';',index=None)
\end{minted}

\begin{verbatim}
(6040, 4)
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.metrics import roc_curve, auc
from sklearn.cross_validation import train_test_split
import os.path,os,logging,datetime,pytz,re, sys
sys.path.append('%s/Downloads/xgboost/wrapper/' % os.environ['HOME'])
import xgboost as xgb

df3 = pd.read_csv('/tmp/customers_clustered.csv',sep=';',index_col='user_id')
X3 = df3.drop('cluster',axis=1)
print X3.shape

aucs = []
for i in range(n_clusters):
    y = (df3['cluster'] == i).astype(float)
    X_train, X_test, y_train, y_test = train_test_split(X3, y, test_size=0.05, random_state=0)
    xg_train = xgb.DMatrix(X_train, label=y_train)
    xg_test = xgb.DMatrix(X_test, label=y_test)    
    watchlist = [ (xg_train,'train'), (xg_test, 'test') ]    
    param = {}; 
    num_round = 1
    param['silent'] = 1
    param['eval_metric'] = 'auc'
    param['max_depth'] = 4
    bst = xgb.train(param, xg_train, num_round, watchlist )
    pred = bst.predict(xg_test)
    fpr, tpr, thresholds = roc_curve(y_test, pred)
    roc_auc = auc(fpr, tpr)
    print 'cluster', i, roc_auc, np.sum(y)
    aucs.append(roc_auc)
    bst.dump_model('/tmp/tree-%d.txt' % i,'/tmp/featmap.txt')
print 'mean auc', np.array(aucs).mean()
\end{minted}

\begin{verbatim}
(6040, 52)
cluster 0 0.975225225225 277.0
cluster 1 0.957269503546 334.0
cluster 2 0.745703971119 407.0
cluster 3 0.807092198582 354.0
cluster 4 0.948563218391 255.0
cluster 5 0.943564162754 339.0
cluster 6 0.969922810753 319.0
cluster 7 0.911730545877 291.0
cluster 8 0.926436781609 265.0
cluster 9 0.990519529769 232.0
cluster 10 0.999155405405 91.0
cluster 11 0.924231150794 238.0
cluster 12 0.987347703843 254.0
cluster 13 0.544214628297 438.0
cluster 14 0.973834325397 307.0
cluster 15 0.785946573751 473.0
cluster 16 0.987572590012 244.0
cluster 17 0.923944805195 282.0
cluster 18 0.901339036638 360.0
cluster 19 0.928017241379 280.0
mean auc 0.906581570417
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
import pickle
all_rules = {}

# Starting from node follow parents all the way to the top to
# determine the "path" that led to that node
def trace(node):
    tr = []
    while node:
        # we start from bottom so dont append, insert in the
        # beginning for the right trace
        tmp = node # if name exists use it
        if tmp in nodes: tmp=nodes.get(node)
        tr.insert(0, (tmp,nodes_yn.get(node)))
        node=parents.get(node)
    return tr

# Walk the tree using recursion, visiting each node, until
# reaching a bottommost leaf, and report the path from root to
# that bottommost leaf. Those paths are our rules.
def walk(T,node):
    if node not in branches:
        rules.append(trace(node))
        return
    left, right = branches[node]
    parents[left] = node
    walk(T, left)
    parents[right] = node
    walk(T, right)

for cluster in range(n_clusters):
    #print 'Tree %d' % cluster
    tree = open('/tmp/tree-%d.txt' % cluster).read()

    nodes_yn = {}
    nodes = re.findall(r'(\d+):\[(.*?)\]',tree,re.DOTALL|re.MULTILINE)
    nodes = dict(x for x in nodes)
    branches = re.findall(r'(\d+):\[.*?(yes=\d+),(no=\d+)',tree,re.DOTALL|re.MULTILINE)
    for x in branches: nodes_yn[x[1].replace('yes=','')] = 'yes'
    for x in branches: nodes_yn[x[2].replace('no=','')] = 'no'
    branches = dict((x[0],(x[1].replace('yes=',''),x[2].replace('no=',''))) for x in branches)
    leaves = re.findall(r'(\d+):leaf=(.*?)\n',tree,re.DOTALL|re.MULTILINE)        
    leaves = dict(x for x in leaves)

    parents = {}; rules = []


    walk(nodes, '0')

    # filter out rules whose bottommost leaf has probability has 0
    #rules = [rule for rule in rules if float( leaves[rule[-1][0]] ) > 0.0]
    new_rules = []
    for rule in rules:
        tmp = [(rule[i][0],rule[i+1][1]) for i in range(len(rule)-1)]
        new_rules.append(tmp)
    all_rules[cluster] = new_rules
    
df = pd.read_csv('/tmp/customers_clustered.csv',sep=';',index_col='user_id')
print 'original dataset', len(df)

rules = all_rules
j = 0
for i in range(n_clusters):
    for r,rule in enumerate(rules[i]): 
        df_slice = df.copy()
        for x in rule: 
            if '<' not in x[0]: 
                df_slice = df_slice[df_slice[x[0]] == int(x[1]=='yes')]
            else:
                a,b = x[0].split('<')
                df_slice = df_slice[(df_slice[a] < float(b)) == (x[1]=='yes') ]

        # only report if slice count is high
        if len(df_slice) > 400:
            print 'Rule', j
            print
            dedup = [rule[i] for i in range(len(rule)) if i == 0 or rule[i] != rule[i-1]]
            for x in rule: print x
            print
            print 'count', len(df_slice)
            print
            j += 1
\end{minted}

\begin{verbatim}
original dataset 6040
Rule 0

('zip2=Michigan/Ohio', 'no')
('occupation=academic/educator', 'no')
('occupation=clerical/admin', 'no')
('zip2=Florida', 'no')

count 4477

Rule 1

('occupation=programmer', 'no')
('occupation=writer', 'no')
('zip2=Northeast', 'yes')
('occupation=other', 'no')

count 530

Rule 2

('occupation=programmer', 'no')
('occupation=writer', 'no')
('zip2=Northeast', 'no')
('Genre_War', 'yes')

count 410

Rule 3

("Genre_Children's", 'no')
('occupation=sales/marketing', 'no')
('occupation=K-12 student', 'no')
('occupation=academic/educator', 'no')

count 633

Rule 4

('occupation=retired', 'no')
('occupation=academic/educator', 'no')
('zip2=Texas / Arkansas', 'no')
('zip2=North', 'yes')

count 582

Rule 5

('occupation=retired', 'no')
('occupation=academic/educator', 'no')
('zip2=Texas / Arkansas', 'no')
('zip2=North', 'no')

count 4408

Rule 6

('occupation=executive/managerial', 'yes')
('zip2=California / Alaska', 'no')
('zip2=North', 'no')
('zip2=Florida', 'no')

count 422

Rule 7

('occupation=executive/managerial', 'no')
('occupation=homemaker', 'no')

count 5269

Rule 8

('occupation=other', 'yes')
('zip2=California / Alaska', 'no')
('zip2=Illinois', 'no')
('zip2=North', 'no')

count 405

Rule 9

('occupation=other', 'no')
('occupation=artist', 'no')
('occupation=scientist', 'no')
('occupation=executive/managerial', 'yes')

count 679

Rule 10

('occupation=other', 'no')
('occupation=artist', 'no')
('occupation=scientist', 'no')
('occupation=executive/managerial', 'no')

count 4239

Rule 11

('zip2=NY Area', 'yes')
('occupation=other', 'no')
('occupation=programmer', 'no')
('occupation=writer', 'no')

count 506

Rule 12

('zip2=NY Area', 'no')
('occupation=writer', 'no')
('occupation=lawyer', 'no')
('zip2=Nevada / Utah', 'no')

count 4740

Rule 13

('occupation=clerical/admin', 'no')
('occupation=tradesman/craftsman', 'no')
('zip2=Texas / Arkansas', 'no')
('zip2=North', 'yes')

count 617

Rule 14

('occupation=clerical/admin', 'no')
('occupation=tradesman/craftsman', 'no')
('zip2=Texas / Arkansas', 'no')
('zip2=North', 'no')

count 4783

Rule 15

('zip2=NY Area', 'no')

count 5378

Rule 16

('occupation=unemployed', 'no')
('occupation=customer service', 'no')

count 5856

Rule 17

('occupation=sales/marketing', 'no')
('occupation=academic/educator', 'no')

count 5210

Rule 18

('zip2=DC', 'no')
('zip2=Michigan/Ohio', 'yes')
('occupation=other', 'no')
('occupation=lawyer', 'no')

count 509

Rule 19

('occupation=customer service', 'no')
('zip2=California / Alaska', 'yes')
('occupation=technician/engineer', 'no')
('occupation=K-12 student', 'no')

count 1270

Rule 20

('occupation=customer service', 'no')
('zip2=California / Alaska', 'no')
('occupation=artist', 'no')
('occupation=retired', 'no')

count 4201

Rule 21

('zip2=Northeast', 'yes')
('occupation=college/grad student', 'no')
('occupation=technician/engineer', 'no')
('occupation=K-12 student', 'no')

count 505

Rule 22

('zip2=Northeast', 'no')
('zip2=Nevada / Utah', 'no')
('zip2=NY Area', 'yes')
('occupation=clerical/admin', 'no')

count 651

Rule 23

('zip2=Northeast', 'no')
('zip2=Nevada / Utah', 'no')
('zip2=NY Area', 'no')

count 4410

Rule 24

('zip2=DC', 'no')
('occupation=academic/educator', 'no')
('zip2=Michigan/Ohio', 'yes')
('occupation=executive/managerial', 'no')

count 492

Rule 25

('zip2=DC', 'no')
('occupation=academic/educator', 'no')
('zip2=Michigan/Ohio', 'no')

count 4576

Rule 26

('zip2=Nevada / Utah', 'no')
('occupation=writer', 'no')
('zip2=Northeast', 'yes')
('occupation=programmer', 'no')

count 601

Rule 27

('zip2=Nevada / Utah', 'no')
('occupation=writer', 'no')
('zip2=Northeast', 'no')
('zip2=NY Area', 'yes')

count 623

Rule 28

('zip2=Nevada / Utah', 'no')
('occupation=writer', 'no')
('zip2=Northeast', 'no')
('zip2=NY Area', 'no')

count 4203

Rule 29

('occupation=self-employed', 'no')
('occupation=college/grad student', 'yes')
('zip2=North', 'no')
('zip2=California / Alaska', 'no')

count 510

Rule 30

('occupation=self-employed', 'no')
('occupation=college/grad student', 'no')
('occupation=technician/engineer', 'no')

count 4538

Rule 31

('occupation=lawyer', 'no')
('zip2=Florida', 'no')
('occupation=homemaker', 'no')
('occupation=executive/managerial', 'yes')

count 621

Rule 32

('occupation=lawyer', 'no')
('zip2=Florida', 'no')
('occupation=homemaker', 'no')
('occupation=executive/managerial', 'no')

count 4826

\end{verbatim}




\url{http://upload.wikimedia.org/wikipedia/commons/2/24/ZIP_Code_zones.svg}

\url{http://www.zipboundary.com/zipcode_faqs.html}

\end{document}
