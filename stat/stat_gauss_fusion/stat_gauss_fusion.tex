\documentclass[12pt,fleqn]{article}\usepackage{../common}
\begin{document}
Algilayicilardan Gelen Gauss Dagilimlarinin Fuzyonu (Gaussian Sensor Fusion)

Diyelim ki tek boyutlu ortamda bir buyuklugu mesela bir lokasyon bilgisi
$x$'i, iki kere olcuyoruz, ve bu olcumu iki degisik algilayiciya
yaptiriyoruz. Yani iki alet bir cismin oldugu yeri bize geri donduruyor, bu
bilgilerde belli olcude gurultu var ki bu belirsizlik aletler yuzunden
olabilir. Diyelim ki iki $z_1,z_2$ olcumu icin iki degisik belirsizlik
(uncertainty) $\sigma_1,\sigma_2$. 

Soru su, bu iki olcumu kullanarak daha iyi bir $x$ tahmini yapabilir miyiz?

Bunun iki olcumu bir sekilde birlestirmemiz gerekiyor. Her olcumu Gaussian
/ Normal dagilim olarak modelleyebiliriz, o zaman iki Normal dagilimi bir
sekilde birlestirmemiz (fusion) lazim. Olcumleri temsil etmek icin Gaussian
bicilmis kaftan. Olcumdeki belirsizligi standart sapma (standart deviation)
uzerinden rahatlikla temsil edebiliriz. Peki birlestirimi nasil yapalim? 

Bu tur problemlerde maksimum olurluk (maximum likelihood) kullanilmasi
gerektigini asagi yukari tahmin edebiliriz, cunku maksimum olurluk verinin
olurlugunu maksimize ederek bilinmeyen parametreleri tahmin etmeye
ugrasir. Cogunlukla bu teknigi hep {\em tek} bir dagilim baglaminda
gormusuzdur herhalde (ders kitaplarinda vs), o tek dagilima degisik veri
noktalari verilerek olasilik sonuclari carpilir, ve bu maksimize edilmeye
ugrasilir. Fakat maksimum olurluk illa tek bir dagilimla kullanilabilir
diye bir kural yok. Ustteki problemde iki olcumu iki Gaussian ile temsil
ederiz (buna mecburuz, iki degisik belirsizlik var), ve bu iki Gaussian'a
verilen iki olcum noktasini olurlugunu bu Gaussian'larin sonuclarini
carparak elde edebiliriz. Peki bilinmeyen $x$ nedir? Onu da Gaussian'in
orta noktasi (mean) olarak aliriz! Yani

$$ L(x) = p(z_1|x,\sigma_1) p(z_2|x,\sigma_2) $$

$$ L(x) \sim \exp{\frac{-(z_1-x)^2}{2\sigma_1^2} } 
\times \frac{-(z_2-x)^2}{2\sigma_2^2} $$

1D Gaussian formulunu hatirlarsak, 

$$ p(z;x,\sigma) = \frac{1}{\sigma\sqrt{2\pi}} 
\exp \bigg\{ - \frac{(z-x)^2}{2\sigma^2}  \bigg\}
 $$

Ders notlari [1]'de iki ustteki formulun nasil maksimize edilerek bir
$x_{MLE}$ formulune erisildigini gorebiliriz. Ustteki formul basindaki
sabit kisminin $L(x)$ formulunde kullanilmadigini goruyoruz, cunku
maksimizasyon acisindan dusunursek, o kisim tekrar tekrar carpilacak ve,
hesaplamaya calistigimiz degiskenler acisindan en azindan bu ek buyukluk
bir fark yaratmayacakti.

Ustteki metot ta isler. Fakat biz alternatif olarak daha degisik (temiz?)
bir yoldan gidecegiz. Elimizdeki her iki olcumu iki farkli 1 boyutlu
Gaussian olarak temsil etmek bir yontemdir. Diger bir yontem su
olabilir. \textbf{2 boyutlu} bir Gaussian yaratiriz, ve iki olcumu tek bir
2 boyutlu vektor icinde belirtiriz, ve tek bir olasilik hesabi $p(z;x,\Sigma)$ 
yapariz. Belirsizlikler ne olacak? Olcum belirsizliklerini bu 2D
Gaussian'in kovaryansinda capraza (diagonal) koyariz, capraz disindaki
matris ogeleri sifir olursa boylece iki olcumun birbirinden bagimsizligini
temsil etmis oluruz. Maksimizasyon nasil olacak? Tek bir olcumun
olasiligini maksimize etmek mumkun, cunku bu hesap sirasinda bilinmeyen
degiskenleri iceren yeni bir formul ortaya cikacak. 

Cok boyutlu Gaussian'i hatirlayalim (artik $z,x$ birer vektor),

$$ p(z;x,\Sigma) = 
\frac{ 1}{(2\pi)^{k/2} \det(\Sigma)^{1/2}} \exp 
\bigg\{ 
-\frac{ 1}{2}(z-x)^T\Sigma^{-1}(z-x)
\bigg\} $$

Kisaca,

$$ =  \frac{ 1}{C} \exp 
\bigg\{ 
-\frac{ 1}{2}(z-x)^T\Sigma^{-1}(z-x)
\bigg\} $$

Bir numara, $\exp$ ve parantez ici negatif ibareden kurtulmak icin
$-\ln p$ alalim,

$$ -\ln p(z) = 
\frac{ 1}{2}(z-x)^T\Sigma^{-1}(z-x)
$$

Simdi iki olcumu, belirsizligi vektor / matris ogeleri olarak gosterelim, 

$$ = \frac{1}{2}  
\left[\begin{array}{c}
z_1-x \\ z_2-x
\end{array}\right]^T
\left[\begin{array}{cc}
\sigma_1^2 & 0 \\
0 & \sigma_2^2 
\end{array}\right]^{-1}
\left[\begin{array}{c}
z_1-x \\ z_2-x
\end{array}\right]
$$

Capraz matrisin tersini almak icin caprazdaki ogelerin tersini almak
yeterlidir,

$$ = \frac{1}{2}  
\left[\begin{array}{c}
z_1-x \\ z_2-x
\end{array}\right]^T
\left[\begin{array}{cc}
\sigma_1^{-2} & 0 \\
0 & \sigma_2^{-2} 
\end{array}\right]
\left[\begin{array}{c}
z_1-x \\ z_2-x
\end{array}\right]
$$

$$ = \frac{1}{2}  
\left[\begin{array}{cc}
\sigma_1^{-2}(z_1-x) & \sigma_2^{-2} (z_2-x)
\end{array}\right]
\left[\begin{array}{c}
z_1-x \\ z_2-x
\end{array}\right]
$$

$$ = 
\frac{1}{2}\sigma_1^{-2}(z_1-x)^2 + \frac{1}{2}\sigma_2^{-2} (z_2-x)^2
$$

Maksimize etmek icin bilinmeyen $x$ degiskenine gore turev alip sifira
esitleyebiliriz, 

$$ 
\sigma_1^{-2}z_1-\sigma_1^{-2}x + \sigma_2^{-2}z_2-\sigma_2^{-2}x = 0
$$

$x$ uzerinden gruplarsak,

$$ 
-x(\sigma_1^{-2}+\sigma_2^{-2}) + \sigma_1^{-2}z_1+ \sigma_2^{-2}z_2 = 0
$$

Gruplanan kismi esitligin sagina alalim,

$$ 
\sigma_1^{-2}z_1+ \sigma_2^{-2}z_2 = x(\sigma_1^{-2}+\sigma_2^{-2}) 
$$

$$ 
\frac{\sigma_1^{-2}z_1+ \sigma_2^{-2}z_2 }{(\sigma_1^{-2}+\sigma_2^{-2})}= x_{MLE}
$$

[1] \url{www.robots.ox.ac.uk/~az/lectures/est/lect34.pdf}


\end{document}
