\documentclass[11pt]{article} 

\usepackage{palatino,eulervm}
\usepackage{showkeys}\renewcommand*\showkeyslabelformat[1]{(#1)}
\usepackage{anysize}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage[latin5]{inputenc}
\usepackage{color}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage[hyphens]{url}
\usepackage[bookmarks=false]{hyperref}
\makeatletter \renewcommand\verbatim@font{\footnotesize\ttfamily} \makeatother
% pseudocode
\usepackage{algorithm}
\newcommand{\code}[1]{\mbox{\texttt{#1}}}
\newcounter{lineno}
\newenvironment{pseudocode}{\begin{small}\begin{tabbing}\textbf{mm}\=mm\=mm\=mm\=mm\=mm\=mm\=mm\=mm\=\kill}{\end{tabbing}\end{small}}
\newcommand{\codename}{\setcounter{lineno}{0}\>}
\newcommand{\codeline}{\>\stepcounter{lineno}\textbf{\arabic{lineno}}\'\>}
\newcommand{\codeskip}{\>\>}

\begin{document}
\title{SVD Factorization for Tall-and-Fat Matrices on Map/Reduce
  Architectures} \author{Burak Bayramlý} \date{October 15, 2013}

\maketitle

\begin{abstract}
  We demonstrate an implementation for an approximate rank-k SVD
  factorization, combining well-known randomized projection techniques with
  previously implemented map/reduce solutions in order to compute steps of
  the random projection based SVD procedure, such QR and SVD. We structure
  the problem in a way that it reduces to Cholesky and SVD factorizations
  on $k \times k$ matrices computed on a single machine, greatly easing the
  computability of the problem.
\end{abstract}

\section{Introduction} \label{intro}

\cite{gleich} presents many excellent techniques for utilizing map/reduce
architectures to compute QR and SVD for the so-called tall-and-skinny
matrices. QR factorization is turned into an $A^TA$ computation problem to
be computed in parallel using map/reduce, and its key element the Cholesky
decomposition, can be performed on a single machine. Let's use $C = A^TA$
and, since

$$ C = A^TA = (QR)^T(QR) = R^TQ^TQR = R^TR $$

and because Cholesky factorization of an $n \times n$ symmetric positive
definite matrix is

$$ C = LL^T $$

where $L$ is an $n \times n$ lower triangular matrix, and R is upper
triangular, we can conclude if we factorize $C$ into $L$ and $L^T$, this
implies $C = LL^T = RR^T$, we have a method of calculating $R$ of QR using
Cholesky factorization on $A^TA$. The key observation here is $A^TA$
computation results an $n \times n$ matrix and if $A$ is ``skinny'' then
$n$ is relatively small (in the thousands), then Cholesky decomposition can
be executed on a small $n \times n$ matrix on a single computer utilizing
an already available function in a scientific computing library. $Q$ is
computed simply as $Q = AR^{-1}$. This again is relatively cheap because R
is $n \times n$, the inverse is computed locallly, matrix multiplication
with $A$ can be performed through map/reduce.

SVD is an additional step. SVD decomposition is 

$$ A = U \Sigma V^T $$

If we expand it with $A = QR$

$$ QR =  U \Sigma V^T $$

$$ R =  Q^T U \Sigma V^T $$

Let's call $\tilde{U} = Q^T U$

$$ R =  \tilde{U} \Sigma V^T $$

This means if we run a local SVD on $R$ (we just calculated above with
Cholesky) which is an $n \times n$ matrix, we will have calculated
$\tilde{U}$, the real $\Sigma$, and real $V^T$. 

Now we have a map/reduce way of calculating QR and SVD on $m \times n$
matrices where $n$ is small.

\subsection{Approximate rank-k SVD}

Switching gears, we look at another method for calculating SVD. The
motivation is while computing SVD, if $n$ is large, creating a ``fat''
matrix which might have columns in the billions would require reducing the
dimensionality of the problem. According to \cite{halko}, one way to
achieve is through random projection. First we draw an $n \times k$
Gaussian random matrix $\Omega$. Then we calculate

$$ Y = A \Omega $$

We perform QR decomposition on $Y$

$$ Y = QR $$

Then form $k \times n$ matrix

$$ B = Q^T A \label{bt} $$

Then we can calculate SVD on this small matrix

$$ B = \hat{U} \Sigma V^T $$

Then form the matrix 

$$ U = Q \hat{U} $$

The main idea is based on

$$ A = QQ^T A $$

if replace $Q$ which comes from random projection $Y$, 

$$ A \approx \tilde{Q}\tilde{Q}^T A $$

$Q$ and $R$ of the projection are close to that of $A$. In the
multiplication above $R$ is called $B$ where $B = \tilde{Q}^T A $, and,

$$ A \approx \tilde{Q}B $$

then, as in \cite{gleich}, we can take SVD of $B$ and apply the same
transition rules to obtain an approximate $U$ of $A$.

This approximation works because of the fact that projecting points to a
random subspace preserves distances between points, or in detail,
projecting the n-point subset onto a random subspace of $O(\log
n/\epsilon^2)$ dimensions only changes the interpoint distances by $(1 \pm
\epsilon)$ with positive probability \cite{gupta}. It is also said that $Y$
is a good representation of the span of $A$.

\subsection{Combining Both Methods}

Our idea was using approximate k-rank SVD calculation steps where $k << n$,
and using map/reduce based QR and SVD methods to implement those steps. By
utilizing random projection, we would be able to work in a smaller
dimension which would translate to local Cholesky, and SVD calls on $k
\times k$ matrices that can be performed in a speedy manner. Below we
outline each map/reduce job.

\begin{algorithm}[h]
\begin{pseudocode}
\codename $\code{random\_projection\_map}(A)$\\
\codeline Tokenize $value$ and pick out id value pairs\\
\codeline result = $\code{zeros}$(1,$k$) \\
\codeline $\code{for each}$ $j^{th}$ $token$ $\in value$ \\
\codeline \> Initialize seed with $j$ \\
\codeline \> $j$ = generate $k$ random numbers \\
\codeline \> $result = result + r \cdot token[j]$ \\
\codeline $\code{emit}$ key, result 
\end{pseudocode}
\end{algorithm}

Reduce is a no-op.

Each value of $A$ will arrive to the algorithm as a key and value pair. Key
is line number or other identifier per row of $A$. Value is a collection of
id value pairs where id is column id this time, and value is the value for
that column. Sparsity is handled through this format, if an id for a column
does not appear in a row of A, it is assumed to be zero. The resulting $Y$
matrix has dimensions $m \times k$.

\begin{algorithm}[h]
\begin{pseudocode}
\codename $A^TA \code{cholesky\_job\_map(key k,value a)}$\\
\codeline $\code{for }$ $i,row$ in $\code{enumerate} a^Ta$  \\
\codeline \> $\code{emit }$ $i,row$ 
\end{pseudocode}
\end{algorithm}

\begin{algorithm}[h]
\begin{pseudocode}
\codename $\code{cholesky\_job\_reduce}(key,value)$\\
\codeline $\code{emit}$ $k,\code{sum}(v_j^k)$  
\end{pseudocode}
\end{algorithm}

\begin{algorithm}[h]
\begin{pseudocode}
\codename $\code{cholesky\_job\_final\_local\_reduce}(key,value)$\\
\codeline $result = \code{cholesky}(A_{sum})$  \\
\codeline $\code{emit }result$ 
\end{pseudocode}
\end{algorithm}

The $\code{cholesky\_job\_final\_local\_reduce}$ step is a function
provided in most map/reduce frameworks, it is a central point that collects
the output of all reducers, naturally a single machine which makes it ideal
to execute the final Cholesky call on by now a very small ($k \times k$)
matrix. The output is $R$.

\begin{algorithm}[h]
\begin{pseudocode}
\codename $\code{Q\_job\_map}(key,value)$\\
\codeline During initialization $R_{inv} = R^{-1}$, store it once for each mapper \\
\codeline $\code{for }$ $row$ in $Y$  \\
\codeline \> $\code{emit }key, row \cdot R_{inv}$ 
\end{pseudocode}
\end{algorithm}

There is no reducer in the Q Job, it is a very simple procedure, it merely
computes multiplication between row of $Y$ and a local matrix $R$. Matrix
$R$ is very small, $k \times k$, hence it can be kept locally in every
node. The $INIT$ function is used to store the inverse of $R$ locally, once
the mapper is initialized, it will always use the same $R^{-1}$ for every
multiplication. 

\begin{thebibliography}{1}

\bibitem{gleich}
Gleich, Benson, Demmel, \emph{Direct QR factorizations for tall-and-skinny
  matrices in MapReduce architectures}, {\tt arXiv:1301.1071 [cs.DC]}, 2013

\bibitem{halko}
N.~Halko, \emph{Randomized methods for computing low-rank approximations of
  matrices}, University of Colorado, Boulder, 2010

\bibitem{gupta}
S.~Dangupta, A.~Gupta \emph{An Elementary Proof of a Theorem of Johnson and
  Lindenstrauss}, Wiley Periodicals, 2002

\bibitem{kurucz}
M.~Kurucz, A. A.~Benczúr, K.~Csalogány, \emph{Methods for large scale SVD with
missing values}, ACM, 2007

\bibitem{bayramli1} B.~Bayramli, \emph{Sasha Framework}, 
\url{git@github.com:burakbayramli/sasha.git}
Github, 2013

\bibitem{bayramli2} B.~Bayramli, \emph{Map/Reduce Code for Netflix SVD Analysis},
  \url{http://github.com/burakbayramli/classnotes/tree/master/stat/stat_mr_rnd_svd/sasha},
  Github, 2013


\end{thebibliography}

\end{document}
