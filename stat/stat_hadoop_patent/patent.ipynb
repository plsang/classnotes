{
 "metadata": {
  "name": "patent"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "source": [
      "Hadoop ile Patent Verisi Islemek\n",
      "\n",
      "75-99 yillari arasinda hangi patentin hangi hangi patentlere referans\n",
      "verdigi ve patentler hakkinda detayli verileri Hadoop ile\n",
      "isleyecegiz. Veriler alttaki baglantidan alinabilir, gerekli dosyalar\n",
      "Dosyalar <code>cite75_99.txt</code> ve <code>apat63_99.txt</code>\n",
      "\n",
      "http://www.nber.org/patents/\n",
      "\n",
      "Referans verisine bakarsak,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 $HOME/Downloads/cite75_99.txt"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"CITING\",\"CITED\"\r\n",
        "3858241,956203\r\n",
        "3858241,1324234\r\n",
        "3858241,3398406\r\n",
        "3858241,3557384\r\n",
        "3858241,3634889\r\n",
        "3858242,1515701\r\n",
        "3858242,3319261\r\n",
        "3858242,3668705\r\n",
        "3858242,3707004\r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "source": [
      "Detayli patent verisine bakalim"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 $HOME/Downloads/apat63_99.txt"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"PATENT\",\"GYEAR\",\"GDATE\",\"APPYEAR\",\"COUNTRY\",\"POSTATE\",\"ASSIGNEE\",\"ASSCODE\",\"CLAIMS\",\"NCLASS\",\"CAT\",\"SUBCAT\",\"CMADE\",\"CRECEIVE\",\"RATIOCIT\",\"GENERAL\",\"ORIGINAL\",\"FWDAPLAG\",\"BCKGTLAG\",\"SELFCTUB\",\"SELFCTLB\",\"SECDUPBD\",\"SECDLWBD\"\r\n",
        "3070801,1963,1096,,\"BE\",\"\",,1,,269,6,69,,1,,0,,,,,,,\r\n",
        "3070802,1963,1096,,\"US\",\"TX\",,1,,2,6,63,,0,,,,,,,,,\r\n",
        "3070803,1963,1096,,\"US\",\"IL\",,1,,2,6,63,,9,,0.3704,,,,,,,\r\n",
        "3070804,1963,1096,,\"US\",\"OH\",,1,,2,6,63,,3,,0.6667,,,,,,,\r\n",
        "3070805,1963,1096,,\"US\",\"CA\",,1,,2,6,63,,1,,0,,,,,,,\r\n",
        "3070806,1963,1096,,\"US\",\"PA\",,1,,2,6,63,,0,,,,,,,,,\r\n",
        "3070807,1963,1096,,\"US\",\"OH\",,1,,623,3,39,,3,,0.4444,,,,,,,\r\n",
        "3070808,1963,1096,,\"US\",\"IA\",,1,,623,3,39,,4,,0.375,,,,,,,\r\n",
        "3070809,1963,1096,,\"US\",\"AZ\",,1,,4,6,65,,0,,,,,,,,,\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "source": [
      "Simdi patent detay verisinden bir orneklem (sample) alalim. Daha ufak bir\n",
      "veri kumesiyle calismak ilk basta faydali olabilir, gelistirme test etme\n",
      "surecini hizlandirir."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!chmod a+r $HOME/Downloads/apat63_99.txt\n",
      "!head -1 $HOME/Downloads/apat63_99.txt > $HOME/Downloads/apat63_99_sampled.txt\n",
      "!cat $HOME/Downloads/apat63_99.txt | perl -n -e 'print if (rand() < .05)' >> $HOME/Downloads/apat63_99_sampled.txt"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Hadoop baslatalim"
     ],
     "language": "python",
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/stop-all.sh\n",
      "!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/start-all.sh"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no jobtracker to stop\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "localhost: no tasktracker to stop\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "no namenode to stop\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "localhost: no datanode to stop\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "localhost: no secondarynamenode to stop\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "starting namenode, logging to /home/hduser/Downloads/hadoop-1.0.4/libexec/../logs/hadoop-hduser-namenode-burak-Aspire-S3.out\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "localhost: starting datanode, logging to /home/hduser/Downloads/hadoop-1.0.4/libexec/../logs/hadoop-hduser-datanode-burak-Aspire-S3.out\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "localhost: starting secondarynamenode, logging to /home/hduser/Downloads/hadoop-1.0.4/libexec/../logs/hadoop-hduser-secondarynamenode-burak-Aspire-S3.out\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "starting jobtracker, logging to /home/hduser/Downloads/hadoop-1.0.4/libexec/../logs/hadoop-hduser-jobtracker-burak-Aspire-S3.out\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "localhost: starting tasktracker, logging to /home/hduser/Downloads/hadoop-1.0.4/libexec/../logs/hadoop-hduser-tasktracker-burak-Aspire-S3.out\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "/home/hduser/Downloads/hadoop*/bin/hadoop dfs -mkdir /user/hduser/patent"
     ],
     "language": "python",
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop dfs -ls /user/hduser/patent"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 2 items\r\n",
        "-rw-r--r--   1 hduser supergroup  236903179 2013-02-21 14:16 /user/hduser/patent/apat63_99.txt\r\n",
        "-rw-r--r--   1 hduser supergroup   11878646 2013-02-21 16:36 /user/hduser/patent/apat63_99_sampled.txt\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop dfs -copyFromLocal /home/burak/Downloads/apat63_99_sampled.txt /user/hduser/patent/apat63_99_sampled.txt"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "copyFromLocal: Target /user/hduser/patent/apat63_99_sampled.txt already exists\r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "source": [
      "Amacimiz patent verisindeki ulke (country) kodunu kullanarak her ulke\n",
      "basina ortalama ne kadar patent uretildigini\n",
      "hesaplamak. Esleme-Indirgeme (Map-Reduce) dongusunde esleme kismini\n",
      "yapacak program asagida."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print open(\"mapper.py\").read()"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#!/usr/bin/python\n",
        "import os,sys\n",
        "os.environ['MPLCONFIGDIR']='/tmp' \n",
        "import pandas as pd\n",
        "data = pd.read_csv(sys.stdin,sep=\",\",index_col=0,usecols=[0,4,8])\n",
        "df = data[pd.notnull(data.ix[:,0]) & pd.notnull(data.ix[:,1])].ix[:,0:2]\n",
        "df.to_csv(sys.stdout,sep=\"\\t\",index=False,header=False)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cp mapper.py /tmp/\n",
      "!chmod a+r /tmp/mapper.py\n",
      "!chmod a+x /tmp/mapper.py"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "source": [
      "Indirgeyici yazmadan once programimizi iki sekilde test edelim. Bu\n",
      "sekillerden birisi hic indirgeyici olmadan, ikincisi\n",
      "<code>IdentityReducer</code> denen kendisine gecilen veriyi oldugu\n",
      "gibi disari atan (ama yine de ortaa bir indirgeyici oldugu icin\n",
      "sonradan bazi islemlerin yine de yapilacagi) seklinde."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop dfs -rmr /user/hduser/output\n",
      "!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop  jar /home/hduser/Downloads/hadoop*/contrib/streaming/hadoop-*streaming*.jar -input patent/apat63_99_sampled.txt  -output output  -mapper /tmp/mapper.py -numReduceTasks 0 "
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Deleted hdfs://localhost:54310/user/hduser/output\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "packageJobJar: [/app/hadoop/tmp/hadoop-unjar2555196345671652661/] [] /tmp/streamjob5013687273729997973.jar tmpDir=null\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 16:30:26 INFO util.NativeCodeLoader: Loaded the native-hadoop library\r\n",
        "13/02/24 16:30:26 WARN snappy.LoadSnappy: Snappy native library not loaded\r\n",
        "13/02/24 16:30:26 INFO mapred.FileInputFormat: Total input paths to process : 1\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 16:30:27 INFO streaming.StreamJob: getLocalDirs(): [/app/hadoop/tmp/mapred/local]\r\n",
        "13/02/24 16:30:27 INFO streaming.StreamJob: Running job: job_201302241611_0012\r\n",
        "13/02/24 16:30:27 INFO streaming.StreamJob: To kill this job, run:\r\n",
        "13/02/24 16:30:27 INFO streaming.StreamJob: /home/hduser/Downloads/hadoop-1.0.4/libexec/../bin/hadoop job  -Dmapred.job.tracker=localhost:54311 -kill job_201302241611_0012\r\n",
        "13/02/24 16:30:27 INFO streaming.StreamJob: Tracking URL: http://localhost:50030/jobdetails.jsp?jobid=job_201302241611_0012\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 16:30:28 INFO streaming.StreamJob:  map 0%  reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 16:30:43 INFO streaming.StreamJob:  map 100%  reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 16:30:49 INFO streaming.StreamJob:  map 100%  reduce 100%\r\n",
        "13/02/24 16:30:49 INFO streaming.StreamJob: Job complete: job_201302241611_0012\r\n",
        "13/02/24 16:30:49 INFO streaming.StreamJob: Output: output\r\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop dfs  -copyToLocal output /tmp/"
     ],
     "language": "python",
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -30 /tmp/output/part-00000"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "FR\t12.0\r\n",
        "US\t5.0\r\n",
        "US\t1.0\r\n",
        "US\t4.0\r\n",
        "US\t4.0\r\n",
        "US\t21.0\r\n",
        "US\t4.0\r\n",
        "US\t8.0\r\n",
        "US\t7.0\r\n",
        "US\t11.0\r\n",
        "DE\t12.0\r\n",
        "US\t30.0\r\n",
        "US\t14.0\r\n",
        "US\t11.0\r\n",
        "US\t5.0\r\n",
        "JP\t21.0\r\n",
        "US\t23.0\r\n",
        "US\t5.0\r\n",
        "CH\t14.0\r\n",
        "DE\t11.0\r\n",
        "US\t4.0\r\n",
        "US\t14.0\r\n",
        "US\t4.0\r\n",
        "US\t1.0\r\n",
        "US\t4.0\r\n",
        "IT\t3.0\r\n",
        "US\t1.0\r\n",
        "US\t7.0\r\n",
        "US\t8.0\r\n",
        "US\t6.0\r\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Ustteki sonucta goruyoruz ki anahtarlar uretilmis, ama ciktilar\n",
      "anahtara gore siralanmamislar. Hatta ustteki sira girdi sirasiyla\n",
      "tipatip ayni. Simdi <code>IdentityReducer</code> uzerinden."
     ],
     "language": "python",
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop dfs -rmr /user/hduser/output\n",
      "!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop  jar /home/hduser/Downloads/hadoop*/contrib/streaming/hadoop-*streaming*.jar -input patent/apat63_99_sampled.txt  -output output  -mapper /tmp/mapper.py -reducer org.apache.hadoop.mapred.lib.IdentityReducer -numReduceTasks 1 "
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Deleted hdfs://localhost:54310/user/hduser/output\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "packageJobJar: [/app/hadoop/tmp/hadoop-unjar2314287838929839696/] [] /tmp/streamjob5231815242060775825.jar tmpDir=null\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 18:03:14 INFO util.NativeCodeLoader: Loaded the native-hadoop library\r\n",
        "13/02/24 18:03:14 WARN snappy.LoadSnappy: Snappy native library not loaded\r\n",
        "13/02/24 18:03:14 INFO mapred.FileInputFormat: Total input paths to process : 1\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 18:03:14 INFO streaming.StreamJob: getLocalDirs(): [/app/hadoop/tmp/mapred/local]\r\n",
        "13/02/24 18:03:14 INFO streaming.StreamJob: Running job: job_201302241759_0004\r\n",
        "13/02/24 18:03:14 INFO streaming.StreamJob: To kill this job, run:\r\n",
        "13/02/24 18:03:14 INFO streaming.StreamJob: /home/hduser/Downloads/hadoop-1.0.4/libexec/../bin/hadoop job  -Dmapred.job.tracker=localhost:54311 -kill job_201302241759_0004\r\n",
        "13/02/24 18:03:14 INFO streaming.StreamJob: Tracking URL: http://localhost:50030/jobdetails.jsp?jobid=job_201302241759_0004\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 18:03:15 INFO streaming.StreamJob:  map 0%  reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 18:03:28 INFO streaming.StreamJob:  map 50%  reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 18:03:31 INFO streaming.StreamJob:  map 100%  reduce 0%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 18:03:40 INFO streaming.StreamJob:  map 100%  reduce 100%\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 18:03:46 INFO streaming.StreamJob: Job complete: job_201302241759_0004\r\n",
        "13/02/24 18:03:46 INFO streaming.StreamJob: Output: output\r\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop dfs  -copyToLocal output /tmp/"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13/02/24 17:59:51 INFO hdfs.DFSClient: No node available for block: blk_-1575974087486179659_1352 file=/user/hduser/output/_logs/history/job_201302241611_0012_1361719827201_hduser_streamjob5013687273729997973.jar\r\n",
        "13/02/24 17:59:51 INFO hdfs.DFSClient: Could not obtain block blk_-1575974087486179659_1352 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...\r\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -30 /tmp/output/part-00000"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AE\t12.0\r\n",
        "AG\t24.0\r\n",
        "AN\t15.0\r\n",
        "AR\t9.0\r\n",
        "AR\t16.0\r\n",
        "AR\t2.0\r\n",
        "AR\t19.0\r\n",
        "AR\t19.0\r\n",
        "AR\t11.0\r\n",
        "AR\t11.0\r\n",
        "AR\t4.0\r\n",
        "AR\t10.0\r\n",
        "AR\t6.0\r\n",
        "AR\t11.0\r\n",
        "AR\t8.0\r\n",
        "AR\t19.0\r\n",
        "AR\t22.0\r\n",
        "AR\t1.0\r\n",
        "AR\t5.0\r\n",
        "AR\t3.0\r\n",
        "AR\t10.0\r\n",
        "AR\t10.0\r\n",
        "AR\t7.0\r\n",
        "AR\t11.0\r\n",
        "AR\t24.0\r\n",
        "AR\t12.0\r\n",
        "AR\t3.0\r\n",
        "AR\t6.0\r\n",
        "AT\t16.0\r\n",
        "AT\t7.0\r\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "source": [
      "Ustteki sonucta anahtarlarin siralanmis oldugunu goruyoruz. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print open(\"reducer.py\").read()"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#!/usr/bin/python\n",
        "import sys\n",
        "(last_key, sum, count) = (None, 0.0, 0)\n",
        "\n",
        "for line in sys.stdin:\n",
        "    (key, val) = line.split(\"\\t\")\n",
        "    if last_key and last_key != key:\n",
        "        print last_key + \"\\t\" + str(sum / count)\n",
        "        (sum, count) = (0.0, 0)\n",
        "    last_key = key\n",
        "    sum += float(val)\n",
        "    count += 1\n",
        "print last_key + \"\\t\" + str(sum / count)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cat /tmp/output/part-00000 | ./reducer.py | tail -10"
     ],
     "language": "python",
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AT\t3.0\r\n",
        "US\t16.0\r\n",
        "JP\t4.0\r\n",
        "SE\t2.0\r\n",
        "GB\t6.5\r\n",
        "US\t13.5\r\n",
        "JP\t18.0\r\n",
        "DE\t8.0\r\n",
        "US\t4.0\r\n",
        "IL\t17.0\r\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "outputs": []
    }
   ]
  }
 ]
}