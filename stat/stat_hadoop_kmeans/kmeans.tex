%% This file was auto-generated by IPython.
%% Conversion from the original notebook file:
%% kmeans.ipynb
%%
\documentclass[11pt,english,fleqn]{article}

%% This is the automatic preamble used by IPython.  Note that it does *not*
%% include a documentclass declaration, that is added at runtime to the overall
%% document.

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}

% needed for markdown enumerations to work
\usepackage{enumerate}

% Slightly bigger margins than the latex defaults
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=2.5cm,rmargin=2.5cm}

% Define a few colors for use in code, links and cell shading
\usepackage{color}
\definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
\definecolor{darkorange}{rgb}{.71,0.21,0.01}
\definecolor{darkgreen}{rgb}{.12,.54,.11}
\definecolor{myteal}{rgb}{.26, .44, .56}
\definecolor{gray}{gray}{0.45}
\definecolor{lightgray}{gray}{.95}
\definecolor{mediumgray}{gray}{.8}
\definecolor{inputbackground}{rgb}{.95, .95, .85}
\definecolor{outputbackground}{rgb}{.95, .95, .95}
\definecolor{traceback}{rgb}{1, .95, .95}

% Framed environments for code cells (inputs, outputs, errors, ...).  The
% various uses of \unskip (or not) at the end were fine-tuned by hand, so don't
% randomly change them unless you're sure of the effect it will have.
\usepackage{framed}

% remove extraneous vertical space in boxes
\setlength\fboxsep{0pt}

% codecell is the whole input+output set of blocks that a Code cell can
% generate.

% TODO: unfortunately, it seems that using a framed codecell environment breaks
% the ability of the frames inside of it to be broken across pages.  This
% causes at least the problem of having lots of empty space at the bottom of
% pages as new frames are moved to the next page, and if a single frame is too
% long to fit on a page, will completely stop latex from compiling the
% document.  So unless we figure out a solution to this, we'll have to instead
% leave the codecell env. as empty.  I'm keeping the original codecell
% definition here (a thin vertical bar) for reference, in case we find a
% solution to the page break issue.

%% \newenvironment{codecell}{%
%%     \def\FrameCommand{\color{mediumgray} \vrule width 1pt \hspace{5pt}}%
%%    \MakeFramed{\vspace{-0.5em}}}
%%  {\unskip\endMakeFramed}

% For now, make this a no-op...
\newenvironment{codecell}{}

 \newenvironment{codeinput}{%
   \def\FrameCommand{\colorbox{inputbackground}}%
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\unskip\endMakeFramed}

\newenvironment{codeoutput}{%
   \def\FrameCommand{\colorbox{outputbackground}}%
   \vspace{-1.4em}
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\unskip\medskip\endMakeFramed}

\newenvironment{traceback}{%
   \def\FrameCommand{\colorbox{traceback}}%
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\endMakeFramed}

% Use and configure listings package for nicely formatted code
\usepackage{listingsutf8}
\lstset{
  language=python,
  inputencoding=utf8x,
  extendedchars=\true,
  aboveskip=\smallskipamount,
  belowskip=\smallskipamount,
  xleftmargin=2mm,
  breaklines=true,
  basicstyle=\small \ttfamily,
  showstringspaces=false,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{myteal},
  stringstyle=\color{darkgreen},
  identifierstyle=\color{darkorange},
  columns=fullflexible,  % tighter character kerning, like verb
}

% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
\usepackage{hyperref}
\hypersetup{
  breaklinks=true,  % so long urls are correctly broken across lines
  colorlinks=true,
  urlcolor=blue,
  linkcolor=darkorange,
  citecolor=darkgreen,
  }

% hardcode size of all verbatim environments to be a bit smaller
\makeatletter 
\g@addto@macro\@verbatim\small\topsep=0.5em\partopsep=0pt
\makeatother 

% Prevent overflowing lines due to urls and other hard-to-break entities.
\sloppy

\setlength{\mathindent}{0pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}
\begin{document}

Paralel KMeans, Hadoop

K-Means algoritmasini nasil paralel sekilde isletiriz? Ozellikle Hadoop
gibi bir Esle-Indirge (Map-Reduce) ortamini dusunelim. Veri cok buyuk
olcekte olabilir ve bu veriler birden fazla makinaya bolunecektir.
Esle-Indirge kavraminda esleme safhasinda ``anahtar uretiriz'', ve sonra
indirgeme safhasinda Hadoop sistemi oyle kurmustur ki ayni anahtarlarlar
tek bir makinaya gonderilir, ve bu nihai asamada artik anahtar bazinda
indirgeme (ozetleme) yapilir.

Paralel K-Means icin anahtar nedir?

Anahtar, mesela kume olabilir. Yani kume 1, kume 2 gibi kume isaretleri
/ sayilari anahtar olarak kullanilabilirler.

Peki anahtar ile eslenecek ``deger'' nedir?

Oyle bir deger ariyoruz ki ust uste konulabilecek bir sey olmali, cunku
EI sisteminin kuvveti burada, anahtarlar farkli noktalarda
uretilebiliyor, sonra tek noktada ust uste konuyor, o zaman degerler
oyle uretilmeli ki bu ust uste koyma, ozetleme islemi yapilabilsin.

Ust uste konabilecek sey kumeye (anahtar) ait olan veri noktasi
olabilir. Normal K-Means'i hatirlarsak, her nokta icin o noktaya en
yakin kumeyi buluyordu sonra, atama islemi bitince, her kumenin
altindaki noktalarin toparlayip, onlarin ortalamasini alarak yeni kume
merkezini hesapliyordu. Bu ortalama islemi ust uste konabilecek bir sey,
yani farkli makinalarda kume-nokta eslemelerini uretirsek, indirgeme
asamasinda o anahtar icin tum degerleri toplayip, nokta sayisina boleriz
ve yeni kume merkezini elde ederiz.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
from IPython.display import Image
Image(filename='kmeans-diag.png')
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
<IPython.core.display.Image at 0xa84e4ac>
\end{verbatim}
\end{codeoutput}
\end{codecell}
Simdi Hadoop ile ilgili bazi lojistik konulara gelelim:

Eger esleme safhasinda her nokta icin en yakin kumeyi bulmak istiyorsak,
o zaman (ilk basta rasgele bile olsa) kume merkezlerinin bilgisi tum
makinalarin erisebilecegi bir yerde olmali. Biz bu veriyi HDFS uzerinde
tutmaya karar verdik, dosya ismi bilinen (well-known) bir yerde olacak,
/tmp/centers.csv.

Paralel K-Means icin tek bir esle-indirge isletimi yeterli degil, bu
algoritma dongulu / ozyineli (iterative) bir algoritma, 5,10,20 kez
islemesi gerekebilir. Her dongu (indirgeme) sonunda yeni kume merkezleri
hesaplanacak, bu merkezler eski /tmp/centers.csv yerini alacak ve islem
tekrar baslayacak.

Dongu sonundaki merkez bilgisi indirgeyicinin ciktisidir ve bu cikti
output/part-00000 dosyasi icinde. Unutmayalim, indirgeyicinin ciktisi
demek, tum indirgeyici makinalardan gelen anahtarlarin (ozetleme
ardindan) birlestirilmesi sonrasinda demek.

Simdi ham veriyi gosterelim, ve Hadoop'u baslatalim.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
from pandas import *
df1 = read_csv("synthetic.txt",sep="   ")
plt.scatter(df1.ix[:,0],df1.ix[:,1])

\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
<matplotlib.collections.PathCollection at 0xafa270c>
\end{verbatim}
\begin{center}
\includegraphics[width=0.7\textwidth]{kmeans_files/kmeans_fig_00.png}
\par
\end{center}
\end{codeoutput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}

!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/stop-all.sh
!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/start-all.sh

\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
no jobtracker to stop
\end{verbatim}
\begin{verbatim}
localhost: no tasktracker to stop
\end{verbatim}
\begin{verbatim}
no namenode to stop
\end{verbatim}
\begin{verbatim}
localhost: no datanode to stop
\end{verbatim}
\begin{verbatim}
localhost: no secondarynamenode to stop
\end{verbatim}
\begin{verbatim}
starting namenode, logging to /home/hduser/Downloads/hadoop-1.0.4/libexec/../logs/hadoop-hduser-namenode-burak-Aspire-S3.out
\end{verbatim}
\begin{verbatim}
localhost: starting datanode, logging to /home/hduser/Downloads/hadoop-1.0.4/libexec/../logs/hadoop-hduser-datanode-burak-Aspire-S3.out
\end{verbatim}
\begin{verbatim}
localhost: starting secondarynamenode, logging to /home/hduser/Downloads/hadoop-1.0.4/libexec/../logs/hadoop-hduser-secondarynamenode-burak-Aspire-S3.out
\end{verbatim}
\begin{verbatim}
starting jobtracker, logging to /home/hduser/Downloads/hadoop-1.0.4/libexec/../logs/hadoop-hduser-jobtracker-burak-Aspire-S3.out
\end{verbatim}
\begin{verbatim}
localhost: starting tasktracker, logging to /home/hduser/Downloads/hadoop-1.0.4/libexec/../logs/hadoop-hduser-tasktracker-burak-Aspire-S3.out
\end{verbatim}
\end{codeoutput}
\end{codecell}
Gerekli merkez verisinin tutulacagi yer /tmp demistik. Bu ismi ozellikle
sectik cunku hem yerel, komut satirindan (Hadoop olmadan) calisirken
rahat kullanilabilecek bir dizin olsun istedik. Bu dizini HDFS uzerinde
yaratalim (unutmayalim, HDFS demek ayri bir alem demek, pur Unix
komutlarimiz bile oraya erisemiyor)

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop dfs -mkdir /tmp/
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
mkdir: cannot create directory /tmp/: File exists
\end{verbatim}
\end{codeoutput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop dfs -ls /
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
Found 3 items
drwxr-xr-x   - hduser supergroup          0 2013-02-25 17:23 /app
drwxr-xr-x   - hduser supergroup          0 2013-02-26 12:49 /tmp
drwxr-xr-x   - hduser supergroup          0 2013-02-26 11:45 /user
\end{verbatim}
\end{codeoutput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
!ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop dfs -copyFromLocal $HOME/Documents/classnotes/stat/stat_hadoop_kmeans/synthetic.txt /user/hduser
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
copyFromLocal: Target /user/hduser/synthetic.txt already exists
\end{verbatim}
\end{codeoutput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
print open("mapper.py").read()
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
#!/usr/bin/python
import os,sys,itertools
import numpy as np
from numpy import linalg as la
os.environ['MPLCONFIGDIR']='/tmp' 
import pandas as pd

centers = pd.read_csv("/tmp/centers.csv",header=None,sep=",")

def dist(vect,x):
    return np.fromiter(itertools.imap(np.linalg.norm, vect-x),dtype=np.float)

def closest(x):
    d = dist(np.array(centers)[:,1:3],np.array(x))
    return np.argmin(d)

comb = lambda x: str(x[0])+":"+str(x[1])

df = pd.read_csv(sys.stdin,header=None,sep="   ")
df['cluster'] = df.apply(closest,axis=1)
df['coord'] = df.apply(comb,axis=1)
df.to_csv(sys.stdout, sep='\t',index=False, cols=['cluster','coord'],
          header=None)
\end{verbatim}
\end{codeoutput}
\end{codecell}
Ustte comb ile kordinat verisini birlestirerek tek kolon haline
getirdik, cunku anahtar deger formunda veri uretmemiz gerekiyor, ve TAB
ayraci sadece tek anahtar ve tek deger ayrimi yapabilir, ve sadece bir
tane ayrac olabilir. Bu sebeple iki boyutlu veri oldugu icin iki sayi
olan deger, yani kordinat : uzerinden birlestirilerek tek bir deger
haline getirildi. Tabii indirgeci bu veriyi alinca bu islemin tersini
yapmasi lazim.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
print open("reducer.py").read()
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
#!/usr/bin/python
import os,sys,itertools
import numpy as np
from numpy import linalg as la
os.environ['MPLCONFIGDIR']='/tmp' 
import pandas as pd

def coords(x):
    return pd.Series(np.array(str(x).split(":"),dtype=np.float64))

df = pd.read_csv(sys.stdin,sep="\t",names=['cluster','coord'])
df2 = df['coord'].apply(coords)
df3 = df.combine_first(df2)
df4 = df3.groupby('cluster').mean()
df4.to_csv(sys.stdout, sep=',',header=None)
\end{verbatim}
\end{codeoutput}
\end{codecell}
Alttaki blokta merkezleri rasgele seciyoruz. Bunun icin veri icinden
rasgele 15 tane (k kadar) nokta cekip cikartik. Bu rasgele sayi uretimi
ile de yapilabilirdi.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
import os,sys,itertools
from numpy import linalg as la
import pandas as pd
k = 15
df = read_csv("synthetic.txt",header=None,sep="   ")
centers = df.take(np.random.permutation(len(df))[:k])
centers.to_csv("/tmp/centers.csv",header=None)
\end{lstlisting}
\end{codeinput}
\end{codecell}
Komut satirindan tek makina icin Hadoop'suz isletelim,

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
for i in range(20):
    os.system("cat synthetic.txt | python mapper.py | python reducer.py > /tmp/centers2.csv")
    os.system("mv /tmp/centers2.csv /tmp/centers.csv")

\end{lstlisting}
\end{codeinput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
df1 = read_csv("synthetic.txt",sep="   ")
plt.scatter(df1.ix[:,0],df1.ix[:,1])
plt.hold(True)
df2 = read_csv("/tmp/centers.csv",header=None,index_col=0)
plt.plot(df2.ix[:,1],df2.ix[:,2],'rd')

\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
[<matplotlib.lines.Line2D at 0xb1ff8cc>]
\end{verbatim}
\begin{center}
\includegraphics[width=0.7\textwidth]{kmeans_files/kmeans_fig_01.png}
\par
\end{center}
\end{codeoutput}
\end{codecell}
Simdi K-Means'i Hadoop'tan isleten kosturucu (runner) program. Bu
program esleme, indirgeme safhalarini cagiracak, ve indirgeyici sonrasi
ciktiyi alip yeni kume merkezi yapacak, bunun gibi idari / temizlik
islerini halledecek.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
os.system("cp mapper.py /tmp/")
os.system("chmod a+r /tmp/mapper.py")
os.system("chmod a+x /tmp/mapper.py")

os.system("cp reducer.py /tmp/")
os.system("chmod a+r /tmp/reducer.py")
os.system("chmod a+x /tmp/reducer.py")

hp_cmd = "ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop"
streaming_cmd = "/home/hduser/Downloads/hadoop*/contrib/streaming/hadoop-*streaming*.jar"

# Ilk seferde merkez verisini HDFS'e yerel dizinden gonder
os.system("%s dfs -rm /user/hduser/centers.csv" % hp_cmd)
os.system("%s dfs -copyFromLocal /tmp/centers.csv /user/hduser" % hp_cmd)

for i in range(20):
    os.system("%s dfs -rmr /user/hduser/output" % hp_cmd)
    os.system("%s jar %s -input synthetic.txt -output output -mapper /tmp/mapper.py -reducer /tmp/reducer.py -numReduceTasks 1" % (hp_cmd,streaming_cmd))
    os.system("%s dfs -cp /user/hduser/output/part-00000 /user/hduser/centers.csv" % hp_cmd) 

\end{lstlisting}
\end{codeinput}
\end{codecell}
Ustte K-Means'i 20 kere islettigimizi goruyoruz. Eger istenirse (hatta
daha iyi olur) dongu bir while icine konur ve bitis icin ``stabilite
sarti'' aranir. Stabilite yeni kume merkezinin eskisinden ``cok fazla
degisik olup olmadigi'' sartidir, degisim yoksa artik sonucu bulmusuz
demektir, daha fazla donguye gerek kalmayacaktir. Biz donguyu 20 kere
donguyu islettik, (bu problem icin) yeterli oldu.

K-Means isini bitirdikten sonra elde edilen sonuclari artik HDFS'ten
yerel dizinimize alabiliriz. Nihai kume merkezleri output/part-00000
icinde. Bu merkezleri alip, ham veri uzerinde kirmizi nokta olarak
gosteriyoruz.

\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
os.system("ssh localhost -l hduser rm /tmp/part-00000")
os.system("ssh localhost -l hduser /home/hduser/Downloads/hadoop*/bin/hadoop dfs -copyToLocal /user/hduser/output/part-00000 /tmp/") # 
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
0
\end{verbatim}
\end{codeoutput}
\end{codecell}
\begin{codecell}
\begin{codeinput}
\begin{lstlisting}
df1 = read_csv("synthetic.txt",sep="   ")
plt.scatter(df1.ix[:,0],df1.ix[:,1])
plt.hold(True)
df2 = read_csv("/tmp/part-00000",header=None,index_col=0)
plt.plot(df2.ix[:,1],df2.ix[:,2],'rd')
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}
\begin{verbatim}
[<matplotlib.lines.Line2D at 0xb369d6c>]
\end{verbatim}
\begin{center}
\includegraphics[width=0.7\textwidth]{kmeans_files/kmeans_fig_02.png}
\par
\end{center}
\end{codeoutput}
\end{codecell}
Sonuclar fena degil. Iste bu metotla terabayt olceginde, devasa bir
veriyi 20-30 makinaya dagitarak parca parca isleyip kumelemeniz
mumkundur. Endustride son zamanlarda habire duyulan Buyuk Veri (Big
Data) olayi iste bu.

\end{document}
